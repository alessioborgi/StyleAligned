{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd StyleAlignedDiffModels\n",
    "%ls\n",
    "\n",
    "# Set up Git configuration\n",
    "!git config --global user.name \"Alessio Borgi\"\n",
    "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
    "\n",
    "# Stage the changes\n",
    "#!git add .\n",
    "\n",
    "# Commit the changes\n",
    "#!git commit -m \"Added some content to your-file.txt\"\n",
    "\n",
    "# Push the changes (replace 'your-token' with your actual personal access token)\n",
    "#!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "import torch\n",
    "import mediapy\n",
    "import sa_handler\n",
    "import pipeline_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models\n",
    "\n",
    "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
    "feature_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "sa_args = sa_handler.StyleAlignedArgs(share_group_norm=False,\n",
    "                                      share_layer_norm=False,\n",
    "                                      share_attention=True,\n",
    "                                      adain_queries=True,\n",
    "                                      adain_keys=True,\n",
    "                                      adain_values=False,\n",
    "                                     )\n",
    "handler = sa_handler.Handler(pipeline)\n",
    "handler.register(sa_args, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get depth maps\n",
    "\n",
    "image = load_image(\"./imgs/train.png\")\n",
    "depth_image1 = pipeline_calls.get_depth_map(image, feature_processor, depth_estimator)\n",
    "depth_image2 = load_image(\"./imgs/sun.png\").resize((1024, 1024))\n",
    "depth_image3 = load_image(\"./imgs/lena.png\").resize((1024, 1024))\n",
    "mediapy.show_images([depth_image1, depth_image2, depth_image3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ControlNet depth with StyleAligned\n",
    "\n",
    "reference_prompt = \"a poster in flat design style\"\n",
    "target_prompts = [\"a train in flat design style\", \"the sun in flat design style\", \"a model in a flat design style\"]\n",
    "controlnet_conditioning_scale = 0.8\n",
    "num_images_per_prompt = 3 # adjust according to VRAM size\n",
    "latents = torch.randn(1 + num_images_per_prompt, 4, 128, 128).to(pipeline.unet.dtype)\n",
    "for deph_map, target_prompt in zip((depth_image1, depth_image2, depth_image3), target_prompts):\n",
    "    latents[1:] = torch.randn(num_images_per_prompt, 4, 128, 128).to(pipeline.unet.dtype)\n",
    "    images = pipeline_calls.controlnet_call(pipeline, [reference_prompt, target_prompt],\n",
    "                                            image=deph_map,\n",
    "                                            num_inference_steps=50,\n",
    "                                            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "                                            num_images_per_prompt=num_images_per_prompt,\n",
    "                                            latents=latents)\n",
    "    \n",
    "    mediapy.show_images([images[0], deph_map] +  images[1:], titles=[\"reference\", \"depth\"] + [f'result {i}' for i in range(1, len(images))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGE MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_map(image: Image) -> Image:\n",
    "    image_np = np.array(image.convert(\"L\"))  # Convert to grayscale\n",
    "    edges = cv2.Canny(image_np, threshold1=100, threshold2=200)\n",
    "    edges_pil = Image.fromarray(edges)\n",
    "    edges_pil = edges_pil.resize((1024, 1024), Image.BICUBIC)\n",
    "    edges_np = np.array(edges_pil)\n",
    "    edges_3_channel = np.stack([edges_np] * 3, axis=-1)\n",
    "    edges_pil = Image.fromarray(edges_3_channel)\n",
    "    return edges_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image = Image.open(\"./path_to_image.jpg\")\n",
    "edge_map = get_edge_map(image)\n",
    "edge_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpose-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpose import pyopenpose as op\n",
    "\n",
    "def get_pose_map(image: Image, openpose_params: dict) -> Image:\n",
    "    # Convert the input image to a NumPy array\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Initialize OpenPose with the given parameters\n",
    "    op_wrapper = op.WrapperPython()\n",
    "    op_wrapper.configure(openpose_params)\n",
    "    op_wrapper.start()\n",
    "\n",
    "    # Prepare the input image for OpenPose\n",
    "    datum = op.Datum()\n",
    "    datum.cvInputData = image_np\n",
    "    op_wrapper.emplaceAndPop([datum])\n",
    "\n",
    "    # Get the pose estimation result\n",
    "    pose_keypoints = datum.poseKeypoints\n",
    "\n",
    "    # Create a blank image to draw the pose skeleton\n",
    "    pose_map = np.zeros_like(image_np)\n",
    "\n",
    "    # Draw the skeleton on the blank image\n",
    "    for person in pose_keypoints:\n",
    "        for keypoint in person:\n",
    "            x, y, confidence = keypoint\n",
    "            if confidence > 0.1:  # Draw only if confidence is high enough\n",
    "                cv2.circle(pose_map, (int(x), int(y)), 5, (255, 255, 255), -1)\n",
    "\n",
    "    # Convert the pose map back to a PIL image\n",
    "    pose_map_pil = Image.fromarray(pose_map)\n",
    "\n",
    "    # Resize to 1024x1024\n",
    "    pose_map_pil = pose_map_pil.resize((1024, 1024), Image.BICUBIC)\n",
    "\n",
    "    return pose_map_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image = Image.open(\"./path_to_image.jpg\")\n",
    "openpose_params = {\n",
    "    \"model_folder\": \"./models/\",\n",
    "    \"net_resolution\": \"-1x256\",\n",
    "    \"hand\": False,\n",
    "    \"face\": False,\n",
    "}\n",
    "pose_map = get_pose_map(image, openpose_params)\n",
    "pose_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIBBLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def get_scribble(image: Image) -> Image:\n",
    "    # Convert the input image to grayscale\n",
    "    image_gray = np.array(image.convert(\"L\"))\n",
    "    \n",
    "    # Detect edges using Canny edge detection\n",
    "    edges = cv2.Canny(image_gray, threshold1=50, threshold2=150)\n",
    "    \n",
    "    # Create a blank image to draw scribbles\n",
    "    scribble = np.zeros_like(edges)\n",
    "    \n",
    "    # Simulate hand-drawn effect by dilating the edges\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    scribble = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Convert the scribble back to an RGB image\n",
    "    scribble_rgb = cv2.cvtColor(scribble, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    scribble_pil = Image.fromarray(scribble_rgb)\n",
    "    \n",
    "    # Resize to 1024x1024\n",
    "    scribble_pil = scribble_pil.resize((1024, 1024), Image.BICUBIC)\n",
    "    \n",
    "    return scribble_pil\n",
    "\n",
    "# Example usage\n",
    "image = Image.open(\"./path_to_image.jpg\")\n",
    "scribble = get_scribble(image)\n",
    "scribble.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_optical_flow_map(image1: Image, image2: Image) -> Image:\n",
    "    # Convert the input images to grayscale\n",
    "    image1_gray = np.array(image1.convert(\"L\"))\n",
    "    image2_gray = np.array(image2.convert(\"L\"))\n",
    "    \n",
    "    # Calculate optical flow using Farneback's algorithm\n",
    "    flow = cv2.calcOpticalFlowFarneback(image1_gray, image2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Convert flow to RGB image\n",
    "    h, w = flow.shape[:2]\n",
    "    flow_map = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Normalize flow vectors\n",
    "    flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    flow_map[..., 0] = flow_angle * 180 / np.pi / 2  # Hue\n",
    "    flow_map[..., 2] = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)  # Value\n",
    "    \n",
    "    # Convert to BGR for cv2 to RGB for PIL\n",
    "    flow_map = cv2.cvtColor(flow_map, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    flow_map_pil = Image.fromarray(flow_map)\n",
    "    \n",
    "    # Resize to 1024x1024\n",
    "    flow_map_pil = flow_map_pil.resize((1024, 1024), Image.BICUBIC)\n",
    "    \n",
    "    return flow_map_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load two consecutive frames from a video or image sequence\n",
    "image1 = Image.open(\"./path_to_first_frame.jpg\")\n",
    "image2 = Image.open(\"./path_to_second_frame.jpg\")\n",
    "\n",
    "# Generate the optical flow map\n",
    "optical_flow_map = get_optical_flow_map(image1, image2)\n",
    "\n",
    "# Display the optical flow map\n",
    "optical_flow_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "\n",
    "def get_point_cloud(image: Image, depth_map: Image, focal_length: float, principal_point: tuple) -> o3d.geometry.PointCloud:\n",
    "    # Convert the image and depth map to NumPy arrays\n",
    "    image_np = np.array(image)\n",
    "    depth_np = np.array(depth_map).astype(np.float32) / 255.0  # Normalize depth map to [0, 1]\n",
    "    \n",
    "    # Get image dimensions\n",
    "    h, w = depth_np.shape[:2]\n",
    "\n",
    "    # Create a mesh grid of pixel coordinates\n",
    "    i, j = np.meshgrid(np.arange(w), np.arange(h), indexing='xy')\n",
    "    \n",
    "    # Convert pixel coordinates to camera coordinates\n",
    "    x = (i - principal_point[0]) * depth_np / focal_length\n",
    "    y = (j - principal_point[1]) * depth_np / focal_length\n",
    "    z = depth_np\n",
    "\n",
    "    # Stack to create a 3D point cloud (h*w, 3)\n",
    "    points = np.stack((x, y, z), axis=-1).reshape(-1, 3)\n",
    "    \n",
    "    # Create Open3D PointCloud object\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "    \n",
    "    # Add colors to the point cloud\n",
    "    colors = image_np.reshape(-1, 3) / 255.0\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "\n",
    "# Load an image and its corresponding depth map\n",
    "image = Image.open(\"./path_to_image.jpg\")\n",
    "depth_map = Image.open(\"./path_to_depth_map.png\")  # Assuming this is a grayscale image\n",
    "\n",
    "# Example camera intrinsic parameters\n",
    "focal_length = 525.0  # Example focal length\n",
    "principal_point = (319.5, 239.5)  # Example principal point (cx, cy)\n",
    "\n",
    "# Generate the 3D point cloud\n",
    "point_cloud = get_point_cloud(image, depth_map, focal_length, principal_point)\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
