{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T12:58:25.754986Z","iopub.execute_input":"2024-05-21T12:58:25.755349Z","iopub.status.idle":"2024-05-21T12:58:26.912448Z","shell.execute_reply.started":"2024-05-21T12:58:25.755321Z","shell.execute_reply":"2024-05-21T12:58:26.911433Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Clone the repository\n!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n\n# Change directory to the cloned repository\n%cd StyleAlignedDiffModels\n\n# Install the required packages\n!pip install -r requirements.txt > /dev/null\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:58:26.914389Z","iopub.execute_input":"2024-05-21T12:58:26.914857Z","iopub.status.idle":"2024-05-21T12:58:46.874915Z","shell.execute_reply.started":"2024-05-21T12:58:26.914825Z","shell.execute_reply":"2024-05-21T12:58:46.873674Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'StyleAlignedDiffModels'...\nremote: Enumerating objects: 26, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (20/20), done.\u001b[K\nremote: Total 26 (delta 7), reused 11 (delta 2), pack-reused 0\u001b[K\nUnpacking objects: 100% (26/26), 13.69 KiB | 1.14 MiB/s, done.\n/kaggle/working/StyleAlignedDiffModels\n","output_type":"stream"}]},{"cell_type":"code","source":"from diffusers import StableDiffusionXLPipeline, DDIMScheduler\nimport torch\ntorch.cuda.empty_cache()\nimport mediapy\nimport sa_handler","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:07:35.751002Z","iopub.execute_input":"2024-05-21T13:07:35.751409Z","iopub.status.idle":"2024-05-21T13:07:35.756520Z","shell.execute_reply.started":"2024-05-21T13:07:35.751380Z","shell.execute_reply":"2024-05-21T13:07:35.755414Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import gc\ndel variables\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# init models\n\nscheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n                              set_alpha_to_one=False)\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True,\n    scheduler=scheduler\n).to(\"cuda\")\n\nhandler = sa_handler.Handler(pipeline)\nsa_args = sa_handler.StyleAlignedArgs(share_group_norm=False,\n                                      share_layer_norm=False,\n                                      share_attention=True,\n                                      adain_queries=True,\n                                      adain_keys=True,\n                                      adain_values=False,\n                                     )\n\nhandler.register(sa_args, )","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:59:06.968949Z","iopub.execute_input":"2024-05-21T12:59:06.970483Z","iopub.status.idle":"2024-05-21T12:59:39.282469Z","shell.execute_reply.started":"2024-05-21T12:59:06.970428Z","shell.execute_reply":"2024-05-21T12:59:39.281582Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af9151b0aeb4a8fbe210db6d0b9df1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade08b671c14475194c42474f3abb1e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1f5a295fde4c8597c175168d66f7b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder_2/config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c7e8afacbc4b7387903a6b26435a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea192ade237b4a85beb750c7810f4529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708632bffb3d4ac388f15874cb9a02f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8855b00d4304a35b6f8beeceb591b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/model.fp16.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b37e0b7ff84ebea5fee3445c6cc9c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95673badc89e480d9abcda6543a3cb82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder_2/model.fp16.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71610676e012458c91a6672b601e1d1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_2/tokenizer_config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933f96f7756f4088b692beb423ee0948"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_2/special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d97c9edd4261444ebc2318e027df982d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_2/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b76d1509394339a66d6089fe6f2689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff068443ab974b43ab1b94b43280cb0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_2/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19bbc9e8dee646dead069ec0c6801c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efda22214a9e4058939f2c6d271a1630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/5.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e90372099e4f2f9e83cc023f6d56d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53503d9df304ae4a13fb75cfcb6fda1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f30f59b8bcc4cb78d932b4ff6d65968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5749046caed34cdc9ed32a7a83de9e42"}},"metadata":{}}]},{"cell_type":"code","source":"# run StyleAligned\ntorch.cuda.memory_summary(device=None, abbreviated=False)\nsets_of_prompts = [\n  \"a toy train. macro photo. 3d game asset\",\n  \"a toy airplane. macro photo. 3d game asset\",\n  \"a toy bicycle. macro photo. 3d game asset\",\n  \"a toy car. macro photo. 3d game asset\",\n  \"a toy boat. macro photo. 3d game asset\",\n]\nimages = pipeline(sets_of_prompts,).images\nmediapy.show_images(images)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:07:42.435924Z","iopub.execute_input":"2024-05-21T13:07:42.436350Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b72439e7314fdda40c5dbbfc3df655"}},"metadata":{}}]},{"cell_type":"markdown","source":"Suggestions for Improving Results\nEnhanced Attention Mechanisms:\n\nMulti-Head Attention Optimization: Fine-tuning the number of attention heads and the way they share information could improve style consistency and reduce content leakage.\nDynamic Attention Sharing: Implementing a dynamic mechanism that adjusts the level of attention sharing based on the complexity of the input text and style might lead to better results.\nAdvanced Normalization Techniques:\n\nLayer-wise AdaIN: Instead of a single AdaIN operation, applying layer-wise adaptive normalization could better capture and transfer intricate style details.\nCombining Normalization Methods: Experiment with combining AdaIN with other normalization techniques like batch normalization or instance normalization to see if it enhances style transfer.\nIncorporating Contextual Information:\n\nContextual Embeddings: Use contextual embeddings from large language models to provide additional information during the attention process, which might improve the alignment of complex styles.\nCross-Modal Attention: Integrate cross-modal attention mechanisms that consider both textual and visual contexts simultaneously, enhancing the model's ability to capture and maintain style consistency.\nArchitectural Modifications\nModular Attention Layers:\n\nPlug-and-Play Attention Modules: Design modular attention layers that can be easily plugged into different parts of the network, allowing for flexible experimentation with where and how attention is shared.\nHierarchical Attention: Implement a hierarchical attention mechanism where different levels of the network focus on different aspects of style and content, ensuring a more granular alignment.\nEnhanced U-Net Architecture:\n\nDeeper U-Net: Extend the depth of the U-Net architecture used in the diffusion process to capture more detailed features at multiple scales, which could improve the richness of the generated styles.\nSkip Connections with Style Information: Incorporate skip connections that carry style information directly from the input to deeper layers of the network, preserving style details more effectively.\nIntegration with Generative Adversarial Networks (GANs):\n\nHybrid Diffusion-GAN Model: Combine the diffusion model with a GAN to leverage the strengths of both architectures. The GAN could help refine the generated images and further enforce style consistency.\nDiscriminator Focused on Style: Train a style-focused discriminator in the GAN setup that specifically evaluates the style consistency of generated images, providing more direct feedback during training.","metadata":{}}]}