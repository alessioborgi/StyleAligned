{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1AXylRlBV_Zx",
        "outputId": "8583c893-942a-4076-dcdf-15cc53a4c99b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StyleAlignedDiffModels'...\n",
            "remote: Enumerating objects: 389, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 389 (delta 59), reused 100 (delta 49), pack-reused 276\u001b[K\n",
            "Receiving objects: 100% (389/389), 106.35 MiB | 11.01 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n",
            "Updating files: 100% (30/30), done.\n",
            "/content/StyleAlignedDiffModels\n",
            "\u001b[0m\u001b[01;34mimgs\u001b[0m/      requirements.txt               StyleAligned_Explanation.ipynb        TO-DO.txt\n",
            "LICENSE    \u001b[01;34msrc\u001b[0m/                           StyleAligned_with_Prompts_only.ipynb\n",
            "README.md  StyleAligned_ControlNet.ipynb  StyleAligned_with_Reference.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
        "\n",
        "# Change directory to the cloned repository\n",
        "%cd StyleAlignedDiffModels\n",
        "%ls\n",
        "\n",
        "# Set up Git configuration\n",
        "!git config --global user.name \"Alessio Borgi\"\n",
        "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
        "\n",
        "# Stage the changes\n",
        "#!git add .\n",
        "\n",
        "# Commit the changes\n",
        "#!git commit -m \"Added some content to your-file.txt\"\n",
        "\n",
        "# Push the changes (replace 'your-token' with your actual personal access token)\n",
        "#!git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V2b06M0aV_Zz",
        "outputId": "641d93e2-13eb-4bd4-f328-c34dcb3e75dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install -r requirements.txt > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fXbZw2evV_Z0",
        "outputId": "59239d7d-f440-4adb-8799-2b8a4d310299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "63febeeb3b724482a8c94923380806c5",
            "f9ee7b08751b480cbed687f586284cfd",
            "1c0645f6e4d7492f83ef93f7fb221174",
            "bfe1a658b5d146a5a68e5f50879e0af7",
            "cab2c7c32e454220ac3cb6c4e5c905e6",
            "4ccdba0e0895413b82887364e200a846",
            "2c3662a126844552b4b2110e566d7f0f",
            "673157abc8994d54a280fea73c9e8214",
            "81c45fe35ead41b9a6c864e1a280320e",
            "f44c2b3658e44c04970c7755b324249d",
            "8123d4c59c93469c98eccfad96e5de1b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63febeeb3b724482a8c94923380806c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pipeline_calls'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ddc246c081d9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmediapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline_calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pipeline_calls'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
        "from diffusers.utils import load_image\n",
        "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
        "import torch\n",
        "import mediapy\n",
        "from src.Handler import Handler\n",
        "from src.StyleAlignedArgs import StyleAlignedArgs\n",
        "import pipeline_calls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa_args = StyleAlignedArgs(share_group_norm=False,\n",
        "                                      share_layer_norm=False,\n",
        "                                      share_attention=True,\n",
        "                                      adain_queries=True,\n",
        "                                      adain_keys=True,\n",
        "                                      adain_values=False,\n",
        "                                     )"
      ],
      "metadata": {
        "id": "91YdrUtAXvPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kuM2HN9V_Z1"
      },
      "outputs": [],
      "source": [
        "# init models\n",
        "\n",
        "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
        "feature_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"diffusers/controlnet-depth-sdxl-1.0\",\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    controlnet=controlnet,\n",
        "    vae=vae,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")\n",
        "pipeline.enable_model_cpu_offload()\n",
        "\n",
        "sa_args = StyleAlignedArgs(share_group_norm=False,\n",
        "                                      share_layer_norm=False,\n",
        "                                      share_attention=True,\n",
        "                                      adain_queries=True,\n",
        "                                      adain_keys=True,\n",
        "                                      adain_values=False,\n",
        "                                     )\n",
        "handler = Handler(pipeline)\n",
        "handler.register(sa_args, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYQQrD6nV_Z1"
      },
      "outputs": [],
      "source": [
        "# get depth maps\n",
        "\n",
        "image = load_image(\"./imgs/train.png\")\n",
        "depth_image1 = pipeline_calls.get_depth_map(image, feature_processor, depth_estimator)\n",
        "depth_image2 = load_image(\"./imgs/sun.png\").resize((1024, 1024))\n",
        "mediapy.show_images([depth_image1, depth_image2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDRq_6r1V_Z2"
      },
      "outputs": [],
      "source": [
        "# run ControlNet depth with StyleAligned\n",
        "\n",
        "reference_prompt = \"a poster in flat design style\"\n",
        "target_prompts = [\"a train in flat design style\", \"the sun in flat design style\"]\n",
        "controlnet_conditioning_scale = 0.8\n",
        "num_images_per_prompt = 3 # adjust according to VRAM size\n",
        "latents = torch.randn(1 + num_images_per_prompt, 4, 128, 128).to(pipeline.unet.dtype)\n",
        "for deph_map, target_prompt in zip((depth_image1, depth_image2), target_prompts):\n",
        "    latents[1:] = torch.randn(num_images_per_prompt, 4, 128, 128).to(pipeline.unet.dtype)\n",
        "    images = pipeline_calls.controlnet_call(pipeline, [reference_prompt, target_prompt],\n",
        "                                            image=deph_map,\n",
        "                                            num_inference_steps=50,\n",
        "                                            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
        "                                            num_images_per_prompt=num_images_per_prompt,\n",
        "                                            latents=latents)\n",
        "\n",
        "    mediapy.show_images([images[0], deph_map] +  images[1:], titles=[\"reference\", \"depth\"] + [f'result {i}' for i in range(1, len(images))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0mGk2WwV_Z2"
      },
      "source": [
        "### ADDITIONAL FUNCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsPVxkr1V_Z4"
      },
      "source": [
        "### EDGE MAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtdfyQnWV_Z4"
      },
      "outputs": [],
      "source": [
        "def get_edge_map(image: Image) -> Image:\n",
        "    image_np = np.array(image.convert(\"L\"))  # Convert to grayscale\n",
        "    edges = cv2.Canny(image_np, threshold1=100, threshold2=200)\n",
        "    edges_pil = Image.fromarray(edges)\n",
        "    edges_pil = edges_pil.resize((1024, 1024), Image.BICUBIC)\n",
        "    edges_np = np.array(edges_pil)\n",
        "    edges_3_channel = np.stack([edges_np] * 3, axis=-1)\n",
        "    edges_pil = Image.fromarray(edges_3_channel)\n",
        "    return edges_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCcvPnshV_Z5"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "image = Image.open(\"./path_to_image.jpg\")\n",
        "edge_map = get_edge_map(image)\n",
        "edge_map.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjPudix3V_Z6"
      },
      "source": [
        "### OPENPOSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6v0BCvrV_Z6"
      },
      "outputs": [],
      "source": [
        "!pip install openpose-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2c2T8JV_Z7"
      },
      "outputs": [],
      "source": [
        "from openpose import pyopenpose as op\n",
        "\n",
        "def get_pose_map(image: Image, openpose_params: dict) -> Image:\n",
        "    # Convert the input image to a NumPy array\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    # Initialize OpenPose with the given parameters\n",
        "    op_wrapper = op.WrapperPython()\n",
        "    op_wrapper.configure(openpose_params)\n",
        "    op_wrapper.start()\n",
        "\n",
        "    # Prepare the input image for OpenPose\n",
        "    datum = op.Datum()\n",
        "    datum.cvInputData = image_np\n",
        "    op_wrapper.emplaceAndPop([datum])\n",
        "\n",
        "    # Get the pose estimation result\n",
        "    pose_keypoints = datum.poseKeypoints\n",
        "\n",
        "    # Create a blank image to draw the pose skeleton\n",
        "    pose_map = np.zeros_like(image_np)\n",
        "\n",
        "    # Draw the skeleton on the blank image\n",
        "    for person in pose_keypoints:\n",
        "        for keypoint in person:\n",
        "            x, y, confidence = keypoint\n",
        "            if confidence > 0.1:  # Draw only if confidence is high enough\n",
        "                cv2.circle(pose_map, (int(x), int(y)), 5, (255, 255, 255), -1)\n",
        "\n",
        "    # Convert the pose map back to a PIL image\n",
        "    pose_map_pil = Image.fromarray(pose_map)\n",
        "\n",
        "    # Resize to 1024x1024\n",
        "    pose_map_pil = pose_map_pil.resize((1024, 1024), Image.BICUBIC)\n",
        "\n",
        "    return pose_map_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWbOl7rXV_Z8"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "image = Image.open(\"./path_to_image.jpg\")\n",
        "openpose_params = {\n",
        "    \"model_folder\": \"./models/\",\n",
        "    \"net_resolution\": \"-1x256\",\n",
        "    \"hand\": False,\n",
        "    \"face\": False,\n",
        "}\n",
        "pose_map = get_pose_map(image, openpose_params)\n",
        "pose_map.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xGIFi0eV_Z8"
      },
      "source": [
        "### SCRIBBLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVIekHsvV_Z9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def get_scribble(image: Image) -> Image:\n",
        "    # Convert the input image to grayscale\n",
        "    image_gray = np.array(image.convert(\"L\"))\n",
        "\n",
        "    # Detect edges using Canny edge detection\n",
        "    edges = cv2.Canny(image_gray, threshold1=50, threshold2=150)\n",
        "\n",
        "    # Create a blank image to draw scribbles\n",
        "    scribble = np.zeros_like(edges)\n",
        "\n",
        "    # Simulate hand-drawn effect by dilating the edges\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    scribble = cv2.dilate(edges, kernel, iterations=1)\n",
        "\n",
        "    # Convert the scribble back to an RGB image\n",
        "    scribble_rgb = cv2.cvtColor(scribble, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    scribble_pil = Image.fromarray(scribble_rgb)\n",
        "\n",
        "    # Resize to 1024x1024\n",
        "    scribble_pil = scribble_pil.resize((1024, 1024), Image.BICUBIC)\n",
        "\n",
        "    return scribble_pil\n",
        "\n",
        "# Example usage\n",
        "image = Image.open(\"./path_to_image.jpg\")\n",
        "scribble = get_scribble(image)\n",
        "scribble.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B8YG3MbV_Z-"
      },
      "source": [
        "### Optical Flow Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRaNy89DV_Z-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def get_optical_flow_map(image1: Image, image2: Image) -> Image:\n",
        "    # Convert the input images to grayscale\n",
        "    image1_gray = np.array(image1.convert(\"L\"))\n",
        "    image2_gray = np.array(image2.convert(\"L\"))\n",
        "\n",
        "    # Calculate optical flow using Farneback's algorithm\n",
        "    flow = cv2.calcOpticalFlowFarneback(image1_gray, image2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    # Convert flow to RGB image\n",
        "    h, w = flow.shape[:2]\n",
        "    flow_map = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "    # Normalize flow vectors\n",
        "    flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    flow_map[..., 0] = flow_angle * 180 / np.pi / 2  # Hue\n",
        "    flow_map[..., 2] = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)  # Value\n",
        "\n",
        "    # Convert to BGR for cv2 to RGB for PIL\n",
        "    flow_map = cv2.cvtColor(flow_map, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    flow_map_pil = Image.fromarray(flow_map)\n",
        "\n",
        "    # Resize to 1024x1024\n",
        "    flow_map_pil = flow_map_pil.resize((1024, 1024), Image.BICUBIC)\n",
        "\n",
        "    return flow_map_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9My6EjAwV_Z-"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load two consecutive frames from a video or image sequence\n",
        "image1 = Image.open(\"./path_to_first_frame.jpg\")\n",
        "image2 = Image.open(\"./path_to_second_frame.jpg\")\n",
        "\n",
        "# Generate the optical flow map\n",
        "optical_flow_map = get_optical_flow_map(image1, image2)\n",
        "\n",
        "# Display the optical flow map\n",
        "optical_flow_map.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM8dJznIV_Z-"
      },
      "source": [
        "### 3D Point Clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeNRsnPCV_Z_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import open3d as o3d\n",
        "\n",
        "def get_point_cloud(image: Image, depth_map: Image, focal_length: float, principal_point: tuple) -> o3d.geometry.PointCloud:\n",
        "    # Convert the image and depth map to NumPy arrays\n",
        "    image_np = np.array(image)\n",
        "    depth_np = np.array(depth_map).astype(np.float32) / 255.0  # Normalize depth map to [0, 1]\n",
        "\n",
        "    # Get image dimensions\n",
        "    h, w = depth_np.shape[:2]\n",
        "\n",
        "    # Create a mesh grid of pixel coordinates\n",
        "    i, j = np.meshgrid(np.arange(w), np.arange(h), indexing='xy')\n",
        "\n",
        "    # Convert pixel coordinates to camera coordinates\n",
        "    x = (i - principal_point[0]) * depth_np / focal_length\n",
        "    y = (j - principal_point[1]) * depth_np / focal_length\n",
        "    z = depth_np\n",
        "\n",
        "    # Stack to create a 3D point cloud (h*w, 3)\n",
        "    points = np.stack((x, y, z), axis=-1).reshape(-1, 3)\n",
        "\n",
        "    # Create Open3D PointCloud object\n",
        "    point_cloud = o3d.geometry.PointCloud()\n",
        "    point_cloud.points = o3d.utility.Vector3dVector(points)\n",
        "\n",
        "    # Add colors to the point cloud\n",
        "    colors = image_np.reshape(-1, 3) / 255.0\n",
        "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "    return point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR-FAch9V_Z_"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import open3d as o3d\n",
        "\n",
        "# Load an image and its corresponding depth map\n",
        "image = Image.open(\"./path_to_image.jpg\")\n",
        "depth_map = Image.open(\"./path_to_depth_map.png\")  # Assuming this is a grayscale image\n",
        "\n",
        "# Example camera intrinsic parameters\n",
        "focal_length = 525.0  # Example focal length\n",
        "principal_point = (319.5, 239.5)  # Example principal point (cx, cy)\n",
        "\n",
        "# Generate the 3D point cloud\n",
        "point_cloud = get_point_cloud(image, depth_map, focal_length, principal_point)\n",
        "\n",
        "# Visualize the point cloud\n",
        "o3d.visualization.draw_geometries([point_cloud])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63febeeb3b724482a8c94923380806c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9ee7b08751b480cbed687f586284cfd",
              "IPY_MODEL_1c0645f6e4d7492f83ef93f7fb221174",
              "IPY_MODEL_bfe1a658b5d146a5a68e5f50879e0af7"
            ],
            "layout": "IPY_MODEL_cab2c7c32e454220ac3cb6c4e5c905e6"
          }
        },
        "f9ee7b08751b480cbed687f586284cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ccdba0e0895413b82887364e200a846",
            "placeholder": "​",
            "style": "IPY_MODEL_2c3662a126844552b4b2110e566d7f0f",
            "value": ""
          }
        },
        "1c0645f6e4d7492f83ef93f7fb221174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_673157abc8994d54a280fea73c9e8214",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81c45fe35ead41b9a6c864e1a280320e",
            "value": 0
          }
        },
        "bfe1a658b5d146a5a68e5f50879e0af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f44c2b3658e44c04970c7755b324249d",
            "placeholder": "​",
            "style": "IPY_MODEL_8123d4c59c93469c98eccfad96e5de1b",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "cab2c7c32e454220ac3cb6c4e5c905e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ccdba0e0895413b82887364e200a846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3662a126844552b4b2110e566d7f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "673157abc8994d54a280fea73c9e8214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "81c45fe35ead41b9a6c864e1a280320e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f44c2b3658e44c04970c7755b324249d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8123d4c59c93469c98eccfad96e5de1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}