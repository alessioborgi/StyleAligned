{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STYLE-ALIGNED WITH CONTROLNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd StyleAlignedDiffModels\n",
    "%ls\n",
    "\n",
    "# Set up Git configuration\n",
    "!git config --global user.name \"Alessio Borgi\"\n",
    "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
    "\n",
    "# Stage the changes\n",
    "#!git add .\n",
    "\n",
    "# Commit the changes\n",
    "#!git commit -m \"Added some content to your-file.txt\"\n",
    "\n",
    "# Push the changes (replace 'your-token' with your actual personal access token)\n",
    "#!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "import einops\n",
    "import mediapy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "from typing import Callable\n",
    "from dataclasses import dataclass\n",
    "from diffusers.utils import load_image\n",
    "from torch.nn import functional as nnf\n",
    "from diffusers.models import attention_processor\n",
    "from diffusers.image_processor import PipelineImageInput\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from diffusers.utils.torch_utils import is_compiled_module, is_torch_version\n",
    "from diffusers import StableDiffusionXLPipeline, DDIMScheduler, ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "\n",
    "from src.Handler import Handler\n",
    "from src.StyleAlignedArgs import StyleAlignedArgs\n",
    "from src.ControlNet import SDXL_ControlNet_Model, concat_zero_control\n",
    "from src.Depth_Map import get_depth_map\n",
    "from src.HarrisCorner import get_edge_map\n",
    "\n",
    "# Create Alias for torch.tensor to increase readability.\n",
    "T = torch.tensor \n",
    "TN = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ControlNet model with specified parameters.\n",
    "ControlNet_Model = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\",  # Model identifier.\n",
    "    variant=\"fp16\",                         # Use 16-bit floating point precision.\n",
    "    use_safetensors=True,                   # Use SafeTensors for security.\n",
    "    torch_dtype=torch.float16               # Set Torch data type to float16.\n",
    ").to(\"cuda\")                                # Move model to GPU.\n",
    "\n",
    "# Load the AutoencoderKL model with specified parameters.\n",
    "AutoencoderKL_Model = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\",        # Model identifier.\n",
    "    torch_dtype=torch.float16               # Set Torch data type to float16.\n",
    ").to(\"cuda\")                                # Move model to GPU.\n",
    "\n",
    "# Initialize the Stable Diffusion XL ControlNet Pipeline\n",
    "SDXL_ControlNet_Pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",  # Model identifier.\n",
    "    controlnet=ControlNet_Model,                       # Attach the loaded ControlNet model.\n",
    "    vae=AutoencoderKL_Model,                                     # Attach the loaded AutoencoderKL model.\n",
    "    variant=\"fp16\",                              # Use 16-bit floating point precision.\n",
    "    use_safetensors=True,                        # Use SafeTensors for security.\n",
    "    torch_dtype=torch.float16                    # Set Torch data type to float16.\n",
    ").to(\"cuda\")                                     # Move pipeline to GPU.\n",
    "\n",
    "# Enable model CPU offload to optimize memory usage.\n",
    "SDXL_ControlNet_Pipeline.enable_model_cpu_offload()\n",
    "\n",
    "# Define Style Aligned Arguments with specified parameters.\n",
    "sa_args = StyleAlignedArgs(\n",
    "    share_group_norm=False,     # Do not share GroupNorm layers.\n",
    "    share_layer_norm=False,     # Do not share LayerNorm layers.\n",
    "    share_attention=True,       # Share Attention layers.\n",
    "    adain_queries=True,         # Apply Adaptive Instance Normalization to queries.\n",
    "    adain_keys=True,            # Apply Adaptive Instance Normalization to keys.\n",
    "    adain_values=False          # Do not apply Adaptive Instance Normalization to values.\n",
    ")\n",
    "\n",
    "# Initialize Handler with the pipeline.\n",
    "handler = Handler(SDXL_ControlNet_Pipeline)\n",
    "\n",
    "# Register the Style Aligned Arguments with the handler.\n",
    "handler.register(sa_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1: CONTROL-NET WITH SIMPLE IMAGE & STYLE-ALIGNMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and resize the control image to 1024x1024 pixels.\n",
    "control_image = load_image(\"./imgs/sun.png\").resize((1024, 1024))\n",
    "\n",
    "# Display the control image using mediapy.\n",
    "mediapy.show_image(control_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference style and prompts for the controlnet.\n",
    "reference_style_controlnet = \"flat design style\"\n",
    "reference_prompt = f\"a poster in {reference_style_controlnet}\"\n",
    "target_prompt = f\"the sun in {reference_style_controlnet}\"\n",
    "\n",
    "# Set the conditioning scale for controlnet.\n",
    "controlnet_conditioning_scale = 0.8\n",
    "\n",
    "# Specify the number of images to generate per prompt.\n",
    "num_images_per_prompt = 3  # Adjust according to VRAM at your disposal.\n",
    "\n",
    "# Generate random latents for the inference process.\n",
    "latents = torch.randn(1 + num_images_per_prompt, 4, 128, 128).to(SDXL_ControlNet_Pipeline.unet.dtype)\n",
    "latents[1:] = torch.randn(num_images_per_prompt, 4, 128, 128).to(SDXL_ControlNet_Pipeline.unet.dtype)\n",
    "\n",
    "# Call the controlnet pipeline to generate images based on the prompts and control image.\n",
    "images_generated = SDXL_ControlNet_Model(SDXL_ControlNet_Pipeline, [reference_prompt, target_prompt],\n",
    "                         image=control_image,\n",
    "                         num_inference_steps=50,\n",
    "                         controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "                         num_images_per_prompt=num_images_per_prompt,\n",
    "                         latents=latents)\n",
    "\n",
    "# Display the generated images along with the control image.\n",
    "mediapy.show_images(\n",
    "    [images_generated[0], control_image] + images_generated[1:],\n",
    "    titles=[\"reference\", \"depth\"] + [f'result {i}' for i in range(1, len(images_generated))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2: CONTROL-NET WITH DEPTH MAP & STYLE-ALIGNMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DPT model for depth estimation and move it to the GPU.\n",
    "DPT_Estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
    "\n",
    "# Load the corresponding image processor for the DPT model.\n",
    "DPT_Feature_Processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "# Load the control image from the specified path.\n",
    "control_image = load_image(\"./imgs/train.png\")\n",
    "\n",
    "# Generate a depth map for the control image using the feature processor and depth estimator.\n",
    "control_depth_image = get_depth_map(control_image, DPT_Feature_Processor, DPT_Estimator)\n",
    "\n",
    "# Display the generated depth map using mediapy.\n",
    "mediapy.show_image(control_depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference style for ControlNet.\n",
    "reference_style_controlnet = \"flat design style\"\n",
    "\n",
    "# Create prompts for reference and target images.\n",
    "reference_prompt = f\"a poster in {reference_style_controlnet}\"  # Prompt for generating the reference image.\n",
    "target_prompt = f\"a train in {reference_style_controlnet}\"      # Prompt for generating the target image.\n",
    "\n",
    "# Set the conditioning scale for ControlNet.\n",
    "controlnet_conditioning_scale = 0.8\n",
    "\n",
    "# Specify the number of images to generate per prompt.\n",
    "num_images_per_prompt = 3  # Adjust according to VRAM size.\n",
    "\n",
    "# Generate random latents for the inference process.\n",
    "latents = torch.randn(1 + num_images_per_prompt, 4, 128, 128).to(SDXL_ControlNet_Pipeline.unet.dtype)\n",
    "latents[1:] = torch.randn(num_images_per_prompt, 4, 128, 128).to(SDXL_ControlNet_Pipeline.unet.dtype)\n",
    "\n",
    "# Call the ControlNet pipeline to generate images based on the prompts and control depth image.\n",
    "images = SDXL_ControlNet_Model(\n",
    "    SDXL_ControlNet_Pipeline,\n",
    "    [reference_prompt, target_prompt],  # Reference and target prompts.\n",
    "    image=control_depth_image,          # Control depth image input.\n",
    "    num_inference_steps=50,             # Number of inference steps.\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,  # Conditioning scale for ControlNet.\n",
    "    num_images_per_prompt=num_images_per_prompt,  # Number of images to generate per prompt.\n",
    "    latents=latents                     # Latents for the inference process.\n",
    ")\n",
    "\n",
    "# Display the generated images along with the control depth image.\n",
    "mediapy.show_images(\n",
    "    [images[0], control_depth_image] + images[1:],  # Reference image, control depth image, and other generated images.\n",
    "    titles=[\"reference\", \"depth\"] + [f'result {i}' for i in range(1, len(images))]  # Titles for each image.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4: CONTROL-NET WITH EDGE MAP (CANNY DETECTOR) & STYLE-ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DPT model for depth estimation and move it to the GPU.\n",
    "DPT_Estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
    "\n",
    "# Load the corresponding image processor for the DPT model.\n",
    "DPT_Feature_Processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "# Load the control image from the specified path.\n",
    "control_image = load_image(\"./imgs/train.png\")\n",
    "\n",
    "# Generate edge map for the control image using the feature processor and depth estimator.\n",
    "control_edge_image = get_edge_map(control_image, DPT_Feature_Processor, DPT_Estimator)\n",
    "\n",
    "# Display the generated depth map using mediapy.\n",
    "mediapy.show_image(control_edge_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference style for ControlNet.\n",
    "reference_style_controlnet = \"flat design style\"\n",
    "\n",
    "# Create prompts for reference and target images.\n",
    "reference_prompt = f\"a poster in {reference_style_controlnet}\"  # Prompt for generating the reference image.\n",
    "target_prompt = f\"a train in {reference_style_controlnet}\"      # Prompt for generating the target image.\n",
    "\n",
    "# Set the conditioning scale for ControlNet.\n",
    "controlnet_conditioning_scale = 0.8\n",
    "\n",
    "# Specify the number of images to generate per prompt.\n",
    "num_images_per_prompt = 3  # Adjust according to VRAM size.\n",
    "\n",
    "# Generate random latents for the inference process.\n",
    "latents = torch.randn(1 + num_images_per_prompt, 4, 128, 128).to(SDXL_ControlNet_Pipeline.unet.dtype)\n",
    "latents[1:] = torch.randn(num_images_per_prompt, 4, 128, 128).to(SDXL_ControlNet_Pipeline.unet.dtype)\n",
    "\n",
    "# Call the ControlNet pipeline to generate images based on the prompts and control edge image.\n",
    "images = SDXL_ControlNet_Model(\n",
    "    SDXL_ControlNet_Pipeline,\n",
    "    [reference_prompt, target_prompt],  # Reference and target prompts.\n",
    "    image=control_edge_image,            # Control edge image input.\n",
    "    num_inference_steps=50,              # Number of inference steps.\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,  # Conditioning scale for ControlNet.\n",
    "    num_images_per_prompt=num_images_per_prompt,  # Number of images to generate per prompt.\n",
    "    latents=latents                     # Latents for the inference process.\n",
    ")\n",
    "\n",
    "# Display the generated images along with the control edge image.\n",
    "mediapy.show_images(\n",
    "    [images[0], control_edge_image] + images[1:],  # Reference image, control edge image, and other generated images.\n",
    "    titles=[\"reference\", \"edge\"] + [f'result {i}' for i in range(1, len(images))]  # Titles for each image.\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
