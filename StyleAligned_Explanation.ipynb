{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540d8642-c203-471c-a66d-0d43aabb0706",
   "metadata": {},
   "source": [
    "# StyleAligned: Zero-Shot Style Alignment among a Series of Generated Images via Attention Sharing\n",
    "\n",
    "### **Authors**: ***Borgi Alessio***, ***Danese Francesco***\n",
    "\n",
    "### **Abstract**\n",
    "In this notebook we aim to reproduce and enhance **[StyleAligned](https://arxiv.org/abs/2312.02133)**, a novel technique introduced by **Google Research**, for achieving **Style Consistency** in large-scale Text-to-Image (T2I) generative models. While current T2I models excel in creating visually compelling images from textual descriptions, they often struggle to maintain a consistent style across multiple images. Traditional methods to address this require extensive fine-tuning and manual intervention. \n",
    "\n",
    "**StyleAligned** addresses this challenge by introducing minimal **Attention Sharing** during the **Diffusion Process**, ensuring **Style Alignment among generated images** without the need for optimization or fine-tuning (**Zero-Shoot Inference**). The method operates by leveraging a straightforward inversion operation to apply a reference style across various generated images, maintaining high-quality synthesis and fidelity to the provided text prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82b07b",
   "metadata": {},
   "source": [
    "### 0: SETTINGS & IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07f09c",
   "metadata": {},
   "source": [
    "#### 0.1: CLONE REPOSITORY AND GIT SETUP\n",
    "\n",
    "In the following cell, we setup the code, by cloning the repository, setting up the Git configurations, and providing some other useful commands useful for git.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd StyleAlignedDiffModels\n",
    "%ls\n",
    "\n",
    "# Set up Git configuration\n",
    "!git config --global user.name \"Alessio Borgi\"\n",
    "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
    "\n",
    "# Stage the changes\n",
    "#!git add .\n",
    "\n",
    "# Commit the changes\n",
    "#!git commit -m \"Added some content to your-file.txt\"\n",
    "\n",
    "# Push the changes (replace 'your-token' with your actual personal access token)\n",
    "#!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f941e8d",
   "metadata": {},
   "source": [
    "#### 0.2: INSTALL AND IMPORT REQUIRED LIBRARIES\n",
    "\n",
    "We proceed then by installing and importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94180e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d54ea7-f7ab-4548-9b10-ece87216dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, DDIMScheduler\n",
    "import torch\n",
    "import mediapy\n",
    "import sa_handler\n",
    "from diffusers.utils import load_image\n",
    "import inversion\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d516363",
   "metadata": {},
   "source": [
    "### 4: DDIM \\& PIPELINE DEFINITION\n",
    "We then proceed to load the **SDXL (Stable Diffusion XL)** Model and configure the **DDIM (Denoising Diffusion Implicit Models) Scheduler**. We then configure the **Pipeline**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734c603",
   "metadata": {},
   "source": [
    "#### 4.1: DDIM SCHEDULER\n",
    "\n",
    "The **DDIM Scheduler** is the component used in diffusion models for generating high-quality samples from noise. It controls the denoising process by defining a schedule for adding and removing noise to and from the data. The scheduler is essential in determining how the model transitions from pure noise to a final, coherent image or other data form.\n",
    "\n",
    "In particular, its parameters are:\n",
    "- **beta_start (float)**: Starting value of beta, the variance of the noise schedule. \n",
    "- **beta_end (float)**: Ending value of beta, the variance of the noise schedule. \n",
    "- **beta_schedule (str)**: The type of schedule for beta. (Possible values: \"linear\", \"scaled_linear\", \"squaredcos_cap_v2\", \"sigmoid\"). \n",
    "- **clip_sample (bool)**: If True, the samples are clipped to [-1, 1]. \n",
    "- **set_alpha_to_one (bool)**: If True, alpha will be set to 1 at the end of the sampling process.\n",
    "- **num_train_timesteps (int)**: The number of diffusion steps used during training. \n",
    "- **timestep_spacing (str)**: The method to space out timesteps.(Possible values: \"linspace\", \"leading\"). \n",
    "- **prediction_type (str)**: The type of prediction model used in the scheduler. (Possible values: \"epsilon\", \"sample\", \"v-prediction\").\n",
    "- **trained_betas (torch.Tensor or None)**: Optional tensor of pre-trained betas to use in the scheduler. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be67335",
   "metadata": {},
   "source": [
    "##### 4.1.1: Diffusion Process\n",
    "\n",
    "The diffusion process involves adding noise to the data over a series of timesteps, which is described by the forward process:\n",
    "\n",
    "$$ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) $$\n",
    "\n",
    "where:\n",
    "- $\\alpha_t$ and $\\beta_t$ are the scaling and noise variance terms, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d7a6a",
   "metadata": {},
   "source": [
    "##### 4.1.2: Reverse Process\n",
    "\n",
    "The reverse process aims to recover the data by denoising it, and is given by:\n",
    "\n",
    "$$ p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I}) $$\n",
    "\n",
    "where:\n",
    "- $\\mu_{\\theta}(\\mathbf{x}_t, t)$ is the predicted mean.\n",
    "- $\\sigma_t$ is the standard deviation of the noise at timestep $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39bf6d4",
   "metadata": {},
   "source": [
    "##### 4.1.3: Beta Schedule\n",
    "\n",
    "The beta values are scheduled over timesteps from `beta_start` to `beta_end`. The schedule can be:\n",
    "- **Linear**: \n",
    "\n",
    "$$ \\beta_t = \\beta_{\\text{start}} + t \\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T} $$\n",
    "\n",
    "- **Scaled Linear**:\n",
    "\n",
    "$$ \\beta_t = \\beta_{\\text{start}} + t \\left(\\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T}\\right)^2 $$\n",
    "\n",
    "- **Sigmoid**:\n",
    "\n",
    "$$ \\beta_t = \\beta_{\\text{start}} + (\\beta_{\\text{end}} - \\beta_{\\text{start}}) \\cdot \\text{sigmoid}(t) $$\n",
    "\n",
    "- **Squared Cosine (squaredcos\\_cap\\_v2)**:\n",
    "\n",
    "$$ \\beta_t = \\beta_{\\text{start}} + 0.5 \\left(1 - \\cos\\left(\\frac{t \\pi}{T}\\right)\\right) (\\beta_{\\text{end}} - \\beta_{\\text{start}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df85710",
   "metadata": {},
   "source": [
    "##### 4.1.4: Inference with DDIM\n",
    "\n",
    "During inference, the denoising process can be described as:\n",
    "\n",
    "$$ \\mathbf{x}_{t-1} = \\sqrt{\\alpha_{t-1}} \\left( \\frac{\\mathbf{x}_t - \\sqrt{1 - \\alpha_t} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)}{\\sqrt{\\alpha_t}} \\right) + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) $$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)$ is the noise predicted by the model.\n",
    "- $\\sigma_t$ is the standard deviation for the timestep $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6f1e6-445f-47bc-b9db-0301caeb7490",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scheduler_linear = DDIMScheduler(\n",
    "    beta_start=0.00085,                 # Starting value of beta\n",
    "    beta_end=0.012,                     # Ending value of beta\n",
    "    beta_schedule=\"scaled_linear\",      # Type of schedule for beta values\n",
    "    clip_sample=False,                  # Whether to clip samples to a specified range\n",
    "    set_alpha_to_one=False,             # Whether to set alpha to one at the end of the process\n",
    "    \n",
    "    num_train_timesteps=1000,           # Number of diffusion steps used during training\n",
    "    timestep_spacing=\"linspace\",        # Method to space out timesteps\n",
    "    prediction_type=\"epsilon\",          # Type of prediction model used in the scheduler\n",
    "    trained_betas=None                  # Optional pre-trained beta values\n",
    ")\n",
    "\n",
    "scheduler = scheduler_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3120b5",
   "metadata": {},
   "source": [
    "### 4.2: SDXL PIPELINE DEFINITION\n",
    "\n",
    "We then proceed to **load** the **pre-trained `StableDiffusionXLPipeline` model** with specific configurations to optimize for GPU memory usage and ensure efficient processing. Below is a breakdown of each parameter and its purpose:\n",
    "\n",
    "- **pretrained_model_name_or_path**: The name or path of the pre-trained model to be loaded. In this example, we use `\"stabilityai/stable-diffusion-xl-base-1.0\"`, which is a pre-trained model available in the Stability AI repository.\n",
    "- **torch_dtype**: Specifies the data type for the model's tensors. Here, `torch.float16` is used to enable mixed precision, which helps reduce memory usage and improve computation speed.\n",
    "- **variant**: Indicates the model variant. `\"fp16\"` is used to specify 16-bit floating point precision, aligning with the `torch_dtype` parameter.\n",
    "- **use_safetensors**: Determines whether to use the `safetensors` library for safe tensor loading. Setting this to `True` ensures safer model loading.\n",
    "- **scheduler**: An instance of the scheduler to be used for the diffusion process. In this example, we use a `DDIMScheduler` instance configured for efficient sampling.\n",
    "- **revision**: Specifies the model version to use. The default value is `None`, which means the latest version will be used.\n",
    "- **use_auth_token**: The authentication token used for accessing private models. The default value is `None`, meaning no authentication is required.\n",
    "- **cache_dir**: The directory where the downloaded model will be cached. The default value is `None`, which uses the default cache directory.\n",
    "- **force_download**: Forces the model to be downloaded even if it exists locally. The default value is `False`.\n",
    "- **resume_download**: Resumes a partial download if available. The default value is `False`.\n",
    "- **proxies**: A dictionary of proxy servers to use. The default value is `None`, meaning no proxies are used.\n",
    "- **local_files_only**: Uses only local files if set to `True`. The default value is `False`.\n",
    "- **device_map**: Specifies device placement for model layers. The default value is `None`, which uses the default placement.\n",
    "- **max_memory**: Specifies the maximum memory allowed for each device. The default value is `None`, meaning no specific memory limit is set.\n",
    "\n",
    "Finally, the model is moved to the GPU for faster computations using `.to(\"cuda\")`.\n",
    "\n",
    "The use of mixed precision (`torch_dtype=torch.float16` and `variant=\"fp16\"`) helps in reducing memory usage and improving performance. This configuration is particularly useful when working with large models and limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785645f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDXL_Pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\",  # The model name or path\n",
    "    torch_dtype=torch.float16,            # Data type for the model's tensors\n",
    "    variant=\"fp16\",                       # Model variant for 16-bit floating point precision (Mixed Precision)\n",
    "    use_safetensors=True,                 # Use the safetensors library for safe tensor loading\n",
    "    scheduler=scheduler,                  # Scheduler instance for the diffusion process\n",
    "    \n",
    "    revision=None,                        # Model version to use, default is None\n",
    "    use_auth_token=None,                  # Authentication token, None means no authentication\n",
    "    cache_dir=None,                       # Directory to cache the downloaded model, None uses default\n",
    "    force_download=False,                 # Force download even if the model exists locally\n",
    "    resume_download=False,                # Resume a partial download if available\n",
    "    proxies=None,                         # Dictionary of proxy servers to use, None means no proxies\n",
    "    local_files_only=False,               # Use only local files if set to True\n",
    "    device_map=None,                      # Device placement for model layers, None uses default placement\n",
    "    max_memory=None                       # Maximum memory allowed for each device, None means no specific limit\n",
    ").to(\"cuda\")                              # Move the model to the GPU for faster computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = sa_handler.Handler(SDXL_Pipeline)\n",
    "sa_args = sa_handler.StyleAlignedArgs(share_group_norm=False,\n",
    "                                      share_layer_norm=False,\n",
    "                                      share_attention=True,\n",
    "                                      adain_queries=True,\n",
    "                                      adain_keys=True,\n",
    "                                      adain_values=False\n",
    "                                     )\n",
    "\n",
    "handler.register(sa_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd6159",
   "metadata": {},
   "source": [
    "### 5: RUNNING STYLE-ALIGNED with A SET OF PROMPTS WITHOUT REFERENCE IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77869b",
   "metadata": {},
   "source": [
    "TO RUN IF YOU HAVE ENOUGH GPU RAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca9256-0ce0-45c3-9cba-68c7eff1452f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run StyleAligned\n",
    "\n",
    "sets_of_prompts = [\n",
    "  \"a toy train. macro photo. 3d game asset\",\n",
    "  \"a toy airplane. macro photo. 3d game asset\",\n",
    "  \"a toy bicycle. macro photo. 3d game asset\",\n",
    "  \"a toy car. macro photo. 3d game asset\",\n",
    "  \"a toy boat. macro photo. 3d game asset\",\n",
    "]\n",
    "images = SDXL_Pipeline(sets_of_prompts,).images\n",
    "mediapy.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6251b8",
   "metadata": {},
   "source": [
    "TO RUN IF YOU HAVEN'T ENOUGH GPU RAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819ad6d-0c19-411f-ba97-199909f64805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run StyleAligned\n",
    "sets_of_prompts = [\n",
    "  \"a toy train. macro photo. 3d game asset\",\n",
    "  \"a toy airplane. macro photo. 3d game asset\",\n",
    "  \"a toy bicycle. macro photo. 3d game asset\",\n",
    "  \"a toy car. macro photo. 3d game asset\",\n",
    "  \"a toy boat. macro photo. 3d game asset\",\n",
    "]\n",
    "# sets_of_prompts = [\n",
    "#   \"a hot hair balloon, simple wooden statue\",\n",
    "#   \"a friendly robot, simple wooden statue\",\n",
    "#   \"a bull, simple wooden statue\",\n",
    "# ]\n",
    "images = []\n",
    "for prompt in sets_of_prompts:\n",
    "    # Generate image for each prompt individually\n",
    "    image = SDXL_Pipeline([prompt]).images[0]\n",
    "    images.append(image)\n",
    "    # Clear CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print Memory summary\n",
    "    # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    \n",
    "mediapy.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1bd22",
   "metadata": {},
   "source": [
    "### 6: STYLE-ALIGNED WITH REFERENCE IMAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e473def",
   "metadata": {},
   "source": [
    "#### 6.1: LOADING AND INVERTING REFERENCE IMAGE  \n",
    "Load a reference image and perform the inversion process to extract latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d034bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_style = \"medieval painting\"\n",
    "src_prompt = f'Man laying in a bed, {src_style}.'\n",
    "image_path = './imgs/medieval-bed.jpeg'\n",
    "\n",
    "num_inference_steps = 50\n",
    "x0 = np.array(load_image(image_path).resize((1024, 1024)))\n",
    "zts = inversion.ddim_inversion(SDXL_Pipeline, x0, src_prompt, num_inference_steps, 2)\n",
    "mediapy.show_image(x0, title=\"innput reference image\", height=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b030817",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    src_prompt,\n",
    "    \"A man working on a laptop\",\n",
    "    \"A man eats pizza\",\n",
    "    \"A woman playig on saxophone\",\n",
    "]\n",
    "\n",
    "# some parameters you can adjust to control fidelity to reference\n",
    "shared_score_shift = np.log(2)  # higher value induces higher fidelity, set 0 for no shift\n",
    "shared_score_scale = 1.0  # higher value induces higher, set 1 for no rescale\n",
    "\n",
    "# for very famouse images consider supressing attention to reference, here is a configuration example:\n",
    "# shared_score_shift = np.log(1)\n",
    "# shared_score_scale = 0.5\n",
    "\n",
    "for i in range(1, len(prompts)):\n",
    "    prompts[i] = f'{prompts[i]}, {src_style}.'\n",
    "\n",
    "handler = sa_handler.Handler(pipeline)\n",
    "sa_args = sa_handler.StyleAlignedArgs(\n",
    "    share_group_norm=True, \n",
    "    share_layer_norm=True, \n",
    "    share_attention=True,\n",
    "    adain_queries=True, \n",
    "    adain_keys=True, \n",
    "    adain_values=False,\n",
    "    shared_score_shift=shared_score_shift, \n",
    "    shared_score_scale=shared_score_scale)\n",
    "handler.register(sa_args)\n",
    "\n",
    "zT, inversion_callback = inversion.make_inversion_callback(zts, offset=5)\n",
    "\n",
    "g_cpu = torch.Generator(device='cpu')\n",
    "g_cpu.manual_seed(10)\n",
    "\n",
    "latents = torch.randn(len(prompts), 4, 128, 128, device='cpu', generator=g_cpu,\n",
    "                      dtype=pipeline.unet.dtype,).to('cuda:0')\n",
    "latents[0] = zT\n",
    "\n",
    "images_a = pipeline(prompts, latents=latents,\n",
    "                    callback_on_step_end=inversion_callback,\n",
    "                    num_inference_steps=num_inference_steps, guidance_scale=10.0).images\n",
    "\n",
    "handler.remove()\n",
    "mediapy.show_images(images_a, titles=[p[:-(len(src_style) + 3)] for p in prompts])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
