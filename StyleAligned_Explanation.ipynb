{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "540d8642-c203-471c-a66d-0d43aabb0706",
      "metadata": {
        "id": "540d8642-c203-471c-a66d-0d43aabb0706"
      },
      "source": [
        "# StyleAligned: Zero-Shot Style Alignment among a Series of Generated Images via Attention Sharing\n",
        "\n",
        "### **Authors**: ***Borgi Alessio***, ***Danese Francesco***\n",
        "\n",
        "### **Abstract**\n",
        "In this notebook we aim to reproduce and enhance **[StyleAligned](https://arxiv.org/abs/2312.02133)**, a novel technique introduced by **Google Research**, for achieving **Style Consistency** in large-scale Text-to-Image (T2I) generative models. While current T2I models excel in creating visually compelling images from textual descriptions, they often struggle to maintain a consistent style across multiple images generated. Traditional methods to address this require extensive fine-tuning and manual intervention.\n",
        "\n",
        "**StyleAligned** addresses this challenge by introducing minimal **Attention Sharing** during the **Diffusion Process**, ensuring **Style Alignment among generated images** without the need for optimization or fine-tuning (**Zero-Shoot Inference**). The method operates by leveraging a straightforward inversion operation to apply a reference style across various generated images, maintaining high-quality synthesis and fidelity to the provided text prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f82b07b",
      "metadata": {
        "id": "7f82b07b"
      },
      "source": [
        "### 0: SETTINGS & IMPORTS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc07f09c",
      "metadata": {
        "id": "cc07f09c"
      },
      "source": [
        "#### 0.1: CLONE REPOSITORY AND GIT SETUP\n",
        "\n",
        "In the following cell, we setup the code, by cloning the repository, setting up the Git configurations, and providing some other useful commands useful for git.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "100b46dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "100b46dd",
        "outputId": "dc0d298e-2268-457e-c8c6-9dcc40dafa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'StyleAlignedDiffModels'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 124 (delta 66), reused 50 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (124/124), 16.40 MiB | 9.91 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "/content/StyleAlignedDiffModels\n",
            "\u001b[0m\u001b[01;34mimgs\u001b[0m/         README.md         \u001b[01;34msrc\u001b[0m/                            Style_Aligned_Transfer_SDXL.ipynb\n",
            "inversion.py  requirements.txt  StyleAligned_Explanation.ipynb  TO-DO.txt\n",
            "LICENSE       sa_handler.py     Style_Aligned_SDXL_old.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
        "\n",
        "# Change directory to the cloned repository\n",
        "%cd StyleAlignedDiffModels\n",
        "%ls\n",
        "\n",
        "# Set up Git configuration\n",
        "!git config --global user.name \"Alessio Borgi\"\n",
        "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
        "\n",
        "# Stage the changes\n",
        "#!git add .\n",
        "\n",
        "# Commit the changes\n",
        "#!git commit -m \"Added some content to your-file.txt\"\n",
        "\n",
        "# Push the changes (replace 'your-token' with your actual personal access token)\n",
        "#!git push origin main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f941e8d",
      "metadata": {
        "id": "5f941e8d"
      },
      "source": [
        "#### 0.2: INSTALL AND IMPORT REQUIRED LIBRARIES\n",
        "\n",
        "We proceed then by installing and importing the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94180e84",
      "metadata": {
        "id": "94180e84"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "!pip install -r requirements.txt > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "23d54ea7-f7ab-4548-9b10-ece87216dc18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "5f7997cd2b0b4136a86e3be938138ef4",
            "169c007fe65c4963bc27aab517b24cb8",
            "995f9f48a02f498f90306eebcb2cda8e",
            "f5eaa36e07c148c3a670a983fa34d2ea",
            "fc1629ba1f0442da9187f7c431317091",
            "1ddcfcc473fe434c96afec08ab6d4c91",
            "354313f61a1c4b6dae6c04379863f839",
            "1250b69ca7854136a792a2fc6280197a",
            "7426a510d3a64b31b4172ed67de9ca01",
            "4ca2f1f6040d4d20937812ed2dee8dd5",
            "f351d052c36848c3b15a50c0e13c9e68"
          ]
        },
        "collapsed": true,
        "id": "23d54ea7-f7ab-4548-9b10-ece87216dc18",
        "outputId": "3574d016-7ea0-4287-90ec-415182548b42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f7997cd2b0b4136a86e3be938138ef4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import einops\n",
        "import mediapy\n",
        "import inversion\n",
        "import sa_handler\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from typing import Callable\n",
        "from dataclasses import dataclass\n",
        "from __future__ import annotations\n",
        "from diffusers.utils import load_image\n",
        "from torch.nn import functional as nnf\n",
        "from diffusers.models import attention_processor\n",
        "from diffusers import StableDiffusionXLPipeline, DDIMScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f340ee7f",
      "metadata": {
        "id": "f340ee7f"
      },
      "source": [
        "### 1: UTILS IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd91c40",
      "metadata": {
        "id": "5dd91c40"
      },
      "source": [
        "#### 1.1: ADAIN MODULE\n",
        "\n",
        "The **[Adaptive Instance Normalization (AdaIN)](https://arxiv.org/abs/1703.06868)** module is essential for **StyleAligned**. This works by first computing the mean and standard deviation of the input feature tensor $x$, independently for each feature map. The mean $\\mu_x$ and standard deviation $\\sigma_x$ are calculated as: $$\\mu_x = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}$$ and $$\\sigma_x = \\sqrt{\\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (x_{ij} - \\mu_x)^2 + \\epsilon}$$ where $H$ and $W$ are the height and width of the feature map, respectively, and $\\epsilon$ is a small constant for numerical stability. These statistics are then matched to those of the style features $y$ by normalizing the input features $x$ and then scaling and shifting them using the style's mean $\\mu_y$ and standard deviation $\\sigma_y$.\n",
        "\n",
        "The transformed feature tensor is given by: $$\\text{AdaIN}(x, y) = \\sigma_y \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right) + \\mu_y$$\n",
        "\n",
        "AdaIN receives a content input $x$ and a style input $y$, and simply aligns the channel wise mean and variance of $x$ to match those of $y$.\n",
        "This process enables the content to adopt the style's statistical properties, facilitating effective style transfer, adding almost no computational cost.\n",
        "\n",
        "In the StyleAligned project, instead of applying this normalization on convolutional-extracted feature maps, we embed it in the self attention layer: the AdaIN module is utilized to normalize the Queries $Q_t$ and Keys $K_t$ of the target image using the Queries $Q_r$ and Keys $K_r$ of the reference image:\n",
        "\n",
        "$$\\hat Q_t = \\text{AdaIN}(Q_t, Q_r) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\hat K_t = \\text{AdaIN}(K_t, K_r)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NOBQz6J6y3gm",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOBQz6J6y3gm",
        "outputId": "dd5d833d-0757-490d-d815-b6a3d9062ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 10, 1, 32])\n",
            "torch.Size([2, 8, 10, 1, 32])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 10, 1, 32])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Ignore\n",
        "'''\n",
        "feat = torch.randn(16, 10, 1, 32)\n",
        "b = feat.shape[0]\n",
        "feat_style = torch.stack((feat[0], feat[b // 2])).unsqueeze(1)\n",
        "print(feat_style.shape)\n",
        "feat_style = feat_style.expand(2, b // 2, *feat.shape[1:])\n",
        "print(feat_style.shape)\n",
        "feat_style.reshape(*feat.shape).shape\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcdqDJlVwmPE",
      "metadata": {
        "id": "fcdqDJlVwmPE"
      },
      "outputs": [],
      "source": [
        "T = torch.tensor # Create Alias for torch.tensor to increase readability\n",
        "\n",
        "def concat_first(feat: T, dim=2) -> T: # takes K or V as input with shape = (batch, heads, tokens, dim_head)\n",
        "    feat_style = expand_first(feat) # creates K/V of the two reference imgs (1th and middle-batch) of shape -> as above\n",
        "    # feat_style holds the K/V of the first image repeated for the first half of the batch,\n",
        "    # and of the middle img repeated for the second half of the batch\n",
        "    return torch.cat((feat, feat_style), dim=dim) # concatenate the real K/V along the \"tokens\" dimensions, so that Q_target pays attention to both target and reference(s) Keys and Values tokens\n",
        "\n",
        "# when this takes mean and std -> input shape: (batch, heads, 1, channels), see below\n",
        "# when it takes K or V -> input shape: (batch, heads, tokens, dim_head), as used in \"concat_first\" above\n",
        "def expand_first(feat: T) -> T:\n",
        "    b = feat.shape[0] # Extract batch size\n",
        "    feat_style = torch.stack((feat[0], feat[b // 2])).unsqueeze(1) # shape: (2, 1, heads, 1, channels), stack the mean (or std) of first and middle images in the batch\n",
        "    feat_style = feat_style.expand(2, b // 2, *feat.shape[1:]) # repeat the mean or std batch/2 times (since we are considering 2 stats, from first and middle img in the batch)\n",
        "    return feat_style.reshape(*feat.shape) # reshape so that first half of batch has assigned the mean/std of the first img, second half of the middle image\n",
        "\n",
        "def calc_mean_std(feat, eps: float = 1e-5) -> tuple[T, T]:  # computes mean and std along number of tokens dimension\n",
        "    feat_std = (feat.var(dim=-2, keepdims=True) + eps).sqrt()\n",
        "    feat_mean = feat.mean(dim=-2, keepdims=True)\n",
        "    return feat_mean, feat_std # output shape: (batch, heads, 1, channels)\n",
        "\n",
        "def adain(feat: T) -> T: # Input shape: (Batch, Heads, #Tokens, Channels), #Tokens is number of \"pixels\" in the feature map, channels = dim_head = token_dim\n",
        "    feat_mean, feat_std = calc_mean_std(feat)\n",
        "    feat_style_mean = expand_first(feat_mean)\n",
        "    feat_style_std = expand_first(feat_std)\n",
        "    feat = (feat - feat_mean) / feat_std  # normalize the feature map\n",
        "    feat = feat * feat_style_std + feat_style_mean  # scale and shift the feature map (reparameterization with reference stats)\n",
        "    return feat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "tPUwicCtnE1y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPUwicCtnE1y",
        "outputId": "d478334f-c098-475c-8ad3-8725190c9761"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (to_q): Linear(in_features=32, out_features=512, bias=False)\n",
              "  (to_k): Linear(in_features=32, out_features=512, bias=False)\n",
              "  (to_v): Linear(in_features=32, out_features=512, bias=False)\n",
              "  (to_out): ModuleList(\n",
              "    (0): Linear(in_features=512, out_features=32, bias=True)\n",
              "    (1): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn = attention_processor.Attention(query_dim=32, dim_head = 64, heads = 8)\n",
        "# both Q and K and V are projected from the input hidden states to a dimension that matches heads * dim_head.\n",
        "# example: Projected from (Batch, tokens, 32) to (Batch, tokens, 512), and then reshaped to (Batch, tokens, 8 = heads, 64).\n",
        "attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WMw349W5D6NJ",
      "metadata": {
        "id": "WMw349W5D6NJ"
      },
      "outputs": [],
      "source": [
        "class DefaultAttentionProcessor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.processor = attention_processor.AttnProcessor2_0() # from diffusers.models import attention_processor\n",
        "\n",
        "    def __call__(self, attn: attention_processor.Attention, hidden_states, encoder_hidden_states=None,\n",
        "                 attention_mask=None, **kwargs):\n",
        "        return self.processor(attn, hidden_states, encoder_hidden_states, attention_mask)\n",
        "\n",
        "class SharedAttentionProcessor(DefaultAttentionProcessor):\n",
        "\n",
        "    def shared_call(\n",
        "            self,\n",
        "            attn: attention_processor.Attention,\n",
        "            hidden_states,\n",
        "            encoder_hidden_states=None,\n",
        "            attention_mask=None,\n",
        "            **kwargs\n",
        "    ):\n",
        "\n",
        "        residual = hidden_states\n",
        "        input_ndim = hidden_states.ndim\n",
        "        if input_ndim == 4:\n",
        "            batch_size, channel, height, width = hidden_states.shape\n",
        "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
        "        batch_size, sequence_length, _ = (\n",
        "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
        "        )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "            # scaled_dot_product_attention expects attention_mask shape to be\n",
        "            # (batch, heads, source_length, target_length)\n",
        "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
        "\n",
        "        if attn.group_norm is not None:\n",
        "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = attn.to_q(hidden_states) # linear layer \"channels\" -> \"heads * dim_heads\"\n",
        "        key = attn.to_k(hidden_states) # same as above\n",
        "        value = attn.to_v(hidden_states) # same as above\n",
        "        inner_dim = key.shape[-1] # get \"heads * dim_heads\" value\n",
        "        head_dim = inner_dim // attn.heads # infer \"dim_head\" by dividing for the number of heads\n",
        "\n",
        "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) # shape all back to (batch, heads, tokens, dim_head)\n",
        "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) # same as above\n",
        "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) # same as above\n",
        "        # if self.step >= self.start_inject:\n",
        "        # Adaptive Normalization of Q and K (and eventually V)\n",
        "        if self.adain_queries:\n",
        "            query = adain(query)\n",
        "        if self.adain_keys:\n",
        "            key = adain(key)\n",
        "        if self.adain_values: # usually false\n",
        "            value = adain(value)\n",
        "        # shared attention layer\n",
        "        # Q, V and K shape = (batch, heads, tokens, dim_head)\n",
        "        if self.share_attention:\n",
        "            key = concat_first(key, -2) # create Keys = [K_t, K_r] -> shape: (batch, heads, 2*tokens, dim_head)\n",
        "            value = concat_first(value, -2) # create Values = [V_t, V_r] -> shape: (batch, heads, 2*tokens, dim_head)\n",
        "            hidden_states = nnf.scaled_dot_product_attention(\n",
        "                query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "            ) # hidden_states output shape -> (batch, heads, tokens, dim_head) since Q not double (.,.,tokens,.) and att = softmax(Q * K'/sqrt(dim_head)) * V\n",
        "        else:\n",
        "            hidden_states = nnf.scaled_dot_product_attention(\n",
        "                query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "            )\n",
        "\n",
        "        # now heads concatenation for later re-projection as in standard Multi Head Attention\n",
        "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim) # transpose -> (b, t, h, d_h); reshape -> (b, t, h*d_h)\n",
        "        hidden_states = hidden_states.to(query.dtype)\n",
        "\n",
        "        # linear proj\n",
        "        hidden_states = attn.to_out[0](hidden_states) # to_out[0] = Linear(in_features = heads * dim_heads, out_features = dim_heads, bias=True)\n",
        "        # dropout\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "        # hidden_states shape -> (batch, tokens, dim_head) = (batch, pixels, channels)\n",
        "        if input_ndim == 4:\n",
        "            # shape it back to a \"feature_maps\" ready for convolution\n",
        "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width) # transpose -> (b, c, pixels); reshape to create an \"image\" -> (b, c, h, w)\n",
        "\n",
        "        if attn.residual_connection:\n",
        "            hidden_states = hidden_states + residual # residuals were initial input hidden_states untouched, before attention mechanismf\n",
        "\n",
        "        hidden_states = hidden_states / attn.rescale_output_factor\n",
        "        return hidden_states\n",
        "\n",
        "    def __call__(self, attn: attention_processor.Attention, hidden_states, encoder_hidden_states=None,\n",
        "                 attention_mask=None, **kwargs):\n",
        "\n",
        "        hidden_states = self.shared_call(attn, hidden_states, hidden_states, attention_mask, **kwargs)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "    def __init__(self, style_aligned_args: StyleAlignedArgs):\n",
        "        super().__init__()\n",
        "        self.share_attention = style_aligned_args.share_attention\n",
        "        self.adain_queries = style_aligned_args.adain_queries\n",
        "        self.adain_keys = style_aligned_args.adain_keys\n",
        "        self.adain_values = style_aligned_args.adain_values\n",
        "        self.shared_score_scale = style_aligned_args.shared_score_scale\n",
        "        self.shared_score_shift = style_aligned_args.shared_score_shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f12c1b",
      "metadata": {
        "id": "e9f12c1b"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class StyleAlignedArgs:\n",
        "    share_group_norm: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to share group normalization across the model.\n",
        "    \"\"\"\n",
        "\n",
        "    share_layer_norm: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to share layer normalization across the model.\n",
        "    \"\"\"\n",
        "\n",
        "    share_attention: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to share attention mechanisms across the model.\n",
        "    \"\"\"\n",
        "\n",
        "    adain_queries: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to apply AdaIN (Adaptive Instance Normalization) to the queries.\n",
        "    \"\"\"\n",
        "\n",
        "    adain_keys: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to apply AdaIN to the keys.\n",
        "    \"\"\"\n",
        "\n",
        "    adain_values: bool = False\n",
        "    \"\"\"\n",
        "    Indicates whether to apply AdaIN to the values.\n",
        "    \"\"\"\n",
        "\n",
        "    only_self_level: float = 0.0\n",
        "    \"\"\"\n",
        "    Level of self-attention to be applied exclusively.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d516363",
      "metadata": {
        "id": "1d516363"
      },
      "source": [
        "### 4: DDIM \\& PIPELINE DEFINITION\n",
        "We then proceed to load the **SDXL (Stable Diffusion XL)** Model and configure the **DDIM (Denoising Diffusion Implicit Models) Scheduler**. We then configure the **Pipeline**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e734c603",
      "metadata": {
        "id": "e734c603"
      },
      "source": [
        "#### 4.1: DDIM SCHEDULER\n",
        "\n",
        "The **DDIM Scheduler** is the component used in diffusion models for generating high-quality samples from noise. It controls the denoising process by defining a schedule for adding and removing noise to and from the data. The scheduler is essential in determining how the model transitions from pure noise to a final, coherent image or other data form.\n",
        "\n",
        "In particular, its parameters are:\n",
        "- **beta_start (float)**: Starting value of beta, the variance of the noise schedule.\n",
        "- **beta_end (float)**: Ending value of beta, the variance of the noise schedule.\n",
        "- **beta_schedule (str)**: The type of schedule for beta. (Possible values: \"linear\", \"scaled_linear\", \"squaredcos_cap_v2\", \"sigmoid\").\n",
        "- **clip_sample (bool)**: If True, the samples are clipped to [-1, 1].\n",
        "- **set_alpha_to_one (bool)**: If True, alpha will be set to 1 at the end of the sampling process.\n",
        "- **num_train_timesteps (int)**: The number of diffusion steps used during training.\n",
        "- **timestep_spacing (str)**: The method to space out timesteps.(Possible values: \"linspace\", \"leading\").\n",
        "- **prediction_type (str)**: The type of prediction model used in the scheduler. (Possible values: \"epsilon\", \"sample\", \"v-prediction\").\n",
        "- **trained_betas (torch.Tensor or None)**: Optional tensor of pre-trained betas to use in the scheduler."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be67335",
      "metadata": {
        "id": "5be67335"
      },
      "source": [
        "##### 4.1.1: DIFFUSION PROCESS\n",
        "\n",
        "The diffusion process involves adding noise to the data over a series of timesteps, which is described by the forward process:\n",
        "\n",
        "$$ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) $$\n",
        "\n",
        "where:\n",
        "- $\\alpha_t$ and $\\beta_t$ are the scaling and noise variance terms, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5d7a6a",
      "metadata": {
        "id": "fd5d7a6a"
      },
      "source": [
        "##### 4.1.2: REVERSE PROCESS\n",
        "\n",
        "The reverse process aims to recover the data by denoising it, and is given by:\n",
        "\n",
        "$$ p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I}) $$\n",
        "\n",
        "where:\n",
        "- $\\mu_{\\theta}(\\mathbf{x}_t, t)$ is the predicted mean.\n",
        "- $\\sigma_t$ is the standard deviation of the noise at timestep $t$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d39bf6d4",
      "metadata": {
        "id": "d39bf6d4"
      },
      "source": [
        "##### 4.1.3: BETA SCHEDULE\n",
        "\n",
        "The beta values are scheduled over timesteps from `beta_start` to `beta_end`. The schedule can be:\n",
        "- **Linear**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + t \\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T} $$\n",
        "\n",
        "- **Scaled Linear**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + t \\left(\\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T}\\right)^2 $$\n",
        "\n",
        "- **Sigmoid**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + (\\beta_{\\text{end}} - \\beta_{\\text{start}}) \\cdot \\text{sigmoid}(t) $$\n",
        "\n",
        "- **Squared Cosine (squaredcos\\_cap\\_v2)**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + 0.5 \\left(1 - \\cos\\left(\\frac{t \\pi}{T}\\right)\\right) (\\beta_{\\text{end}} - \\beta_{\\text{start}}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df85710",
      "metadata": {
        "id": "6df85710"
      },
      "source": [
        "##### 4.1.4: INFERENCE WITH DDIM\n",
        "\n",
        "During inference, the denoising process can be described as:\n",
        "\n",
        "$$ \\mathbf{x}_{t-1} = \\sqrt{\\alpha_{t-1}} \\left( \\frac{\\mathbf{x}_t - \\sqrt{1 - \\alpha_t} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)}{\\sqrt{\\alpha_t}} \\right) + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) $$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)$ is the noise predicted by the model.\n",
        "- $\\sigma_t$ is the standard deviation for the timestep $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f6f1e6-445f-47bc-b9db-0301caeb7490",
      "metadata": {
        "id": "c2f6f1e6-445f-47bc-b9db-0301caeb7490",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "scheduler_linear = DDIMScheduler(\n",
        "    beta_start=0.00085,                 # Starting value of beta\n",
        "    beta_end=0.012,                     # Ending value of beta\n",
        "    beta_schedule=\"scaled_linear\",      # Type of schedule for beta values\n",
        "    clip_sample=False,                  # Whether to clip samples to a specified range\n",
        "    set_alpha_to_one=False,             # Whether to set alpha to one at the end of the process\n",
        "\n",
        "    num_train_timesteps=1000,           # Number of diffusion steps used during training\n",
        "    timestep_spacing=\"linspace\",        # Method to space out timesteps\n",
        "    prediction_type=\"epsilon\",          # Type of prediction model used in the scheduler\n",
        "    trained_betas=None                  # Optional pre-trained beta values\n",
        ")\n",
        "\n",
        "scheduler = scheduler_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c3120b5",
      "metadata": {
        "id": "5c3120b5"
      },
      "source": [
        "### 4.2: SDXL PIPELINE DEFINITION\n",
        "\n",
        "We then proceed to **load** the **pre-trained `StableDiffusionXLPipeline` model** with specific configurations to optimize for GPU memory usage and ensure efficient processing. Below is a breakdown of each parameter and its purpose:\n",
        "\n",
        "- **pretrained_model_name_or_path**: The name or path of the pre-trained model to be loaded. In this example, we use `\"stabilityai/stable-diffusion-xl-base-1.0\"`, which is a pre-trained model available in the Stability AI repository.\n",
        "- **torch_dtype**: Specifies the data type for the model's tensors. Here, `torch.float16` is used to enable mixed precision, which helps reduce memory usage and improve computation speed.\n",
        "- **variant**: Indicates the model variant. `\"fp16\"` is used to specify 16-bit floating point precision, aligning with the `torch_dtype` parameter.\n",
        "- **use_safetensors**: Determines whether to use the `safetensors` library for safe tensor loading. Setting this to `True` ensures safer model loading.\n",
        "- **scheduler**: An instance of the scheduler to be used for the diffusion process. In this example, we use a `DDIMScheduler` instance configured for efficient sampling.\n",
        "- **revision**: Specifies the model version to use. The default value is `None`, which means the latest version will be used.\n",
        "- **use_auth_token**: The authentication token used for accessing private models. The default value is `None`, meaning no authentication is required.\n",
        "- **cache_dir**: The directory where the downloaded model will be cached. The default value is `None`, which uses the default cache directory.\n",
        "- **force_download**: Forces the model to be downloaded even if it exists locally. The default value is `False`.\n",
        "- **resume_download**: Resumes a partial download if available. The default value is `False`.\n",
        "- **proxies**: A dictionary of proxy servers to use. The default value is `None`, meaning no proxies are used.\n",
        "- **local_files_only**: Uses only local files if set to `True`. The default value is `False`.\n",
        "- **device_map**: Specifies device placement for model layers. The default value is `None`, which uses the default placement.\n",
        "- **max_memory**: Specifies the maximum memory allowed for each device. The default value is `None`, meaning no specific memory limit is set.\n",
        "\n",
        "Finally, the model is moved to the GPU for faster computations using `.to(\"cuda\")`.\n",
        "\n",
        "The use of mixed precision (`torch_dtype=torch.float16` and `variant=\"fp16\"`) helps in reducing memory usage and improving performance. This configuration is particularly useful when working with large models and limited GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785645f0",
      "metadata": {
        "id": "785645f0"
      },
      "outputs": [],
      "source": [
        "SDXL_Pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\",  # The model name or path\n",
        "    torch_dtype=torch.float16,            # Data type for the model's tensors\n",
        "    variant=\"fp16\",                       # Model variant for 16-bit floating point precision (Mixed Precision)\n",
        "    use_safetensors=True,                 # Use the safetensors library for safe tensor loading\n",
        "    scheduler=scheduler,                  # Scheduler instance for the diffusion process\n",
        "\n",
        "    revision=None,                        # Model version to use, default is None\n",
        "    use_auth_token=None,                  # Authentication token, None means no authentication\n",
        "    cache_dir=None,                       # Directory to cache the downloaded model, None uses default\n",
        "    force_download=False,                 # Force download even if the model exists locally\n",
        "    resume_download=False,                # Resume a partial download if available\n",
        "    proxies=None,                         # Dictionary of proxy servers to use, None means no proxies\n",
        "    local_files_only=False,               # Use only local files if set to True\n",
        "    device_map=None,                      # Device placement for model layers, None uses default placement\n",
        "    max_memory=None                       # Maximum memory allowed for each device, None means no specific limit\n",
        ").to(\"cuda\")                              # Move the model to the GPU for faster computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb08ebfe",
      "metadata": {
        "id": "bb08ebfe"
      },
      "outputs": [],
      "source": [
        "handler = sa_handler.Handler(SDXL_Pipeline)\n",
        "sa_args = sa_handler.StyleAlignedArgs(share_group_norm=False,\n",
        "                                      share_layer_norm=False,\n",
        "                                      share_attention=True,\n",
        "                                      adain_queries=True,\n",
        "                                      adain_keys=True,\n",
        "                                      adain_values=False\n",
        "                                     )\n",
        "\n",
        "handler.register(sa_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbdd6159",
      "metadata": {
        "id": "fbdd6159"
      },
      "source": [
        "### 5: RUNNING STYLE-ALIGNED with A SET OF PROMPTS WITHOUT REFERENCE IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e77869b",
      "metadata": {
        "id": "7e77869b"
      },
      "source": [
        "TO RUN IF YOU HAVE ENOUGH GPU RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cca9256-0ce0-45c3-9cba-68c7eff1452f",
      "metadata": {
        "id": "5cca9256-0ce0-45c3-9cba-68c7eff1452f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# run StyleAligned\n",
        "\n",
        "sets_of_prompts = [\n",
        "  \"a toy train. macro photo. 3d game asset\",\n",
        "  \"a toy airplane. macro photo. 3d game asset\",\n",
        "  \"a toy bicycle. macro photo. 3d game asset\",\n",
        "  \"a toy car. macro photo. 3d game asset\",\n",
        "  \"a toy boat. macro photo. 3d game asset\",\n",
        "]\n",
        "images = SDXL_Pipeline(sets_of_prompts,).images\n",
        "mediapy.show_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6251b8",
      "metadata": {
        "id": "bc6251b8"
      },
      "source": [
        "TO RUN IF YOU HAVEN'T ENOUGH GPU RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d819ad6d-0c19-411f-ba97-199909f64805",
      "metadata": {
        "id": "d819ad6d-0c19-411f-ba97-199909f64805"
      },
      "outputs": [],
      "source": [
        "# run StyleAligned\n",
        "sets_of_prompts = [\n",
        "  \"a toy train. macro photo. 3d game asset\",\n",
        "  \"a toy airplane. macro photo. 3d game asset\",\n",
        "  \"a toy bicycle. macro photo. 3d game asset\",\n",
        "  \"a toy car. macro photo. 3d game asset\",\n",
        "  \"a toy boat. macro photo. 3d game asset\",\n",
        "]\n",
        "# sets_of_prompts = [\n",
        "#   \"a hot hair balloon, simple wooden statue\",\n",
        "#   \"a friendly robot, simple wooden statue\",\n",
        "#   \"a bull, simple wooden statue\",\n",
        "# ]\n",
        "images = []\n",
        "for prompt in sets_of_prompts:\n",
        "    # Generate image for each prompt individually\n",
        "    image = SDXL_Pipeline([prompt]).images[0]\n",
        "    images.append(image)\n",
        "    # Clear CUDA cache to free memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Print Memory summary\n",
        "    # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "mediapy.show_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c1bd22",
      "metadata": {
        "id": "d7c1bd22"
      },
      "source": [
        "### 6: STYLE-ALIGNED WITH REFERENCE IMAGE\n",
        "\n",
        "In this section, we develop the **StyleAligned** method that **maintains visual coherence** w.r.t. a **Reference Style Image**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e473def",
      "metadata": {
        "id": "9e473def"
      },
      "source": [
        "#### 6.1: LOADING REFERENCE IMAGE & SETTING PARAMETERS\n",
        "\n",
        "As first thing, we **load a Reference Image** from which you will \"copy the style\". Indeed, we will ask for the newly generated images to stick with the same style of the Reference image, i.e., to be **Style-Aligned**.\n",
        "In particular, we will define 3 **Image Parameters** here:\n",
        "- **reference_style**: This is the reference style describing the reference image.\n",
        "- **reference_prompt**: This is the reference prompt describing the reference image.\n",
        "- **reference_image_path**: This is the path to the reference image.\n",
        "\n",
        "As second step, you will set the parameters relative to the **Diffusion Inversion Process**. This process in a.k.a as **Temperature Scaling**, that aims to inject confidence/randomness in a classification/generation model. In this case, we aim at injecting more styleAlignment or randomness to the image generated.\n",
        "\n",
        "In particular, you will set the parameters relative to:\n",
        "- **num_inference_steps**: The number of inference steps to be performed during the Diffusion Inversion Process.\n",
        "- **guidance_scale**: Here we set the parameter to have **Guidance Scale**, a parameter used in **guided diffusion models** to control the influence of the conditioning signal (e.g., a text prompt) during the image generation process, with the purpose to balance the model’s adherence to the conditioning signal and its natural generative tendencies. In particular:\n",
        "\t- **High Guidance Scale (>1)**: Increases the influence of the conditioning signal, making the generated images more aligned with the prompt. The model is more likely to produce images with features that are explicitly described in the prompt, leading to more detailed and specific outputs. Very high values might cause the model to overfit to the prompt, potentially losing some naturalness or introducing artifacts.\n",
        "\t- **Default Guidance Scale (=1)**: A guidance scale of 1.0 means that the model’s predictions are equally balanced between the conditional and unconditional signals, providing a baseline level of adherence to the prompt.\n",
        "\t- **Low Guidance Scale (<1)**: A lower guidance scale reduces the effect of the guiding input, making the generated images less constrained by the prompt. The model has more freedom to generate diverse and potentially more creative outputs that are not strictly bound to the prompt. Very low values might cause the model to generate images that are too generic and not sufficiently aligned with the prompt.\n",
        "- **style_alignment_score_shift**: This parameter is used to **shift** the **scores logarithmically**. In particular, we will have:\n",
        "\t- **High Values (>1)**: This will **increase** the **StyleAlignment**, making therefore the output image to be closer to the reference.\n",
        "\t- **Low Values (<=1)**: This will **decrease** the **StyleAlignment**, making therefore the output image to be farther to the reference.\n",
        "- **style_alignment_score_scale**: This parameter is used to **scale** the scores or weights within the model. More in detail:\n",
        "\t- **High Values (>1)**: This increases the magnitude of the scores, making the model more confident and therefore not varying so much the generation process.\n",
        "\t- **Standard Value (=1)**: This translates in having no rescaling.\n",
        "\t- **Low Values (<1)**: This coincides to having more generalization (injecting randomness) into the generation process.\n",
        "\n",
        "***Special Configuration for Famous Images***\n",
        "\n",
        "For very famous images, it might be beneficial to suppress the **Shared Attention** to the reference image to avoid overfitting or excessive influence from the reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d034bfe",
      "metadata": {
        "id": "5d034bfe"
      },
      "outputs": [],
      "source": [
        "# 1) Medieval Painting\n",
        "# Set the source style, prompt and path.\n",
        "reference_style = \"medieval painting\"\n",
        "reference_prompt = f'Man laying in a bed, {reference_style}.'\n",
        "reference_image_path = './imgs/medieval-bed.jpeg'\n",
        "\n",
        "# 2) Cubism Painting\n",
        "# reference_style = \"cubism painting\"\n",
        "# reference_prompt = f'Two men smoking water pipe, {reference_style}.'\n",
        "# reference_image_path = './imgs/Picasso_Smoking_Water_Pipe.jpeg'\n",
        "\n",
        "\n",
        "# Setting the number of inference steps in the Diffusion Inversion Process.\n",
        "num_inference_steps = 50\n",
        "\n",
        "# Setting the Guidance Scale for the Diffusion Inversion Process.\n",
        "guidance_scale = 10.0\n",
        "\n",
        "# 1) Normal Painting\n",
        "# These are some parameters you can Adjust to Control StyleAlignment to Reference Image.\n",
        "style_alignment_score_shift = 2  # higher value induces higher fidelity, set 0 for no shift\n",
        "style_alignment_score_scale = 1.0  # higher value induces higher, set 1 for no rescale\n",
        "\n",
        "# 2) Very Famous Paintings\n",
        "# style_alignment_score_shift = 1\n",
        "# style_alignment_score_scale = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32dc685",
      "metadata": {
        "id": "c32dc685"
      },
      "outputs": [],
      "source": [
        "# Load the reference image and resize it to 1024x1024 pixels.\n",
        "ref_image = np.array(load_image(reference_image_path).resize((1024, 1024)))\n",
        "\n",
        "# Display the output image.\n",
        "mediapy.show_image(ref_image, title=\"Reference Image for Style Alignment\", height=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a2a01c",
      "metadata": {},
      "source": [
        "#### 6.2: FUNCTION FOR PROMPT TOKENIZATION & EMBEDDING \n",
        "\n",
        "Hereafter, we aim to create the function that, taking a **text prompt** in **input**, **tokenizes** it through a Tokenizer, and then generates **embeddings** using a text encoder. \n",
        "\n",
        "It will then return in output both the **Hidden State Embeddings** (contextual representations for each token from the second last layer of the text encoder) and the **Pooled Output Embeddings** (single vector representation capturing the overall meaning of the input text). If the prompt is empty, it returns zero tensors as negative embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f78b735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alias for torch.Tensor type\n",
        "T = torch.Tensor\n",
        "\n",
        "# Alias for a type that can be either a torch.Tensor or None\n",
        "TN = T | None\n",
        "\n",
        "# Defining a type alias for the Diffusion Inversion Process type of callable.\n",
        "InversionCallback = Callable[[StableDiffusionXLPipeline, int, T, dict[str, T]], dict[str, T]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5b4067",
      "metadata": {},
      "source": [
        "The function we will develop for this is going to receive as Parameters: \n",
        "- ***prompt***: A string containing the text prompt to be tokenized and embedded.\n",
        "- ***tokenizer***: An instance of a tokenizer, used to convert the text prompt into token IDs.\n",
        "- ***text_encoder***: An instance of a text encoder, used to generate embeddings from the token IDs.\n",
        "- ***device***: The device (e.g., CPU or GPU) to run the computations on.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Tokenize the Input Prompt**: Converts the input text (prompt) into a sequence of tokens, which are numerical representations (token IDs) that can be processed by the model.\n",
        "- **2) Extract Token IDs**: Extracts the token IDs from the tokenized inputs.\n",
        "- **3) Generate Embeddings**: Generates embeddings for the tokenized input using the text encoder.\n",
        "- **4) Extract Pooled Output Embeddings**: Extracts the pooled output embeddings, summarizing the input sequence.\n",
        "- **5) Extract Hidden State Embeddings**: Extracts the hidden state embeddings from the second last layer of the text encoder.\n",
        "- **6) Handle Empty Prompt Case**: Checks if the input prompt is empty and handles this case.\n",
        "- **7) Return Embeddings**: Returns the generated embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c87a50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_text_embeddings(prompt: str, tokenizer, text_encoder, device):\n",
        "\n",
        "    # 1) Tokenize the Input Prompt: Tokenize the input prompt using the provided tokenizer, with padding and truncation.\n",
        "    text_inputs = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\n",
        "    # 2) Extract Token IDs: Extract the input IDs (token indices) from the tokenized inputs.\n",
        "    text_input_ids = text_inputs.input_ids\n",
        "\n",
        "    # 3) Generate Embeddings: Use torch.no_grad() to disable gradient computation for the following operations.\n",
        "    with torch.no_grad():\n",
        "        # Generate embeddings for the tokenized input IDs using the text encoder.\n",
        "        # The embeddings include output hidden states.\n",
        "        prompt_embeds = text_encoder(\n",
        "            text_input_ids.to(device),  # Move input IDs to the specified device (e.g., GPU).\n",
        "            output_hidden_states=True,  # Request hidden states from the encoder.\n",
        "        )\n",
        "\n",
        "    # 4) Extract Pooled Output Embeddings: Extract the pooled output embeddings (first element of the tuple returned by the encoder).\n",
        "    pooled_prompt_embeds = prompt_embeds[0]\n",
        "    # 5) Extract Hidden State Embeddings: Extract the hidden state embeddings from the second last layer of the encoder.\n",
        "    prompt_embeds = prompt_embeds.hidden_states[-2]\n",
        "    \n",
        "    # 6) Handle Empty Prompt Case: If the prompt is empty, return zero tensors as negative embeddings.\n",
        "    if prompt == '':\n",
        "        # Create a zero tensor with the same shape as the hidden state embeddings.\n",
        "        negative_prompt_embeds = torch.zeros_like(prompt_embeds)\n",
        "        # Create a zero tensor with the same shape as the pooled output embeddings.\n",
        "        negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)\n",
        "        # Return the zero tensors for both negative embeddings and pooled negative embeddings.\n",
        "        return negative_prompt_embeds, negative_pooled_prompt_embeds\n",
        "    \n",
        "    # 7) Returns the generated embeddings: Return the hidden state embeddings and the pooled output embeddings.\n",
        "    return prompt_embeds, pooled_prompt_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1f2366",
      "metadata": {},
      "source": [
        "#### 6.4: TEXT EMBEDDING ENSEMBLE METHOD\n",
        "\n",
        "In order to **enhance** the **model’s understanding** and **representation** of the **input text**, we proceed to generate and process text embeddings for a given prompt using **2 Different Text Tokenizers and Encoders**. We then prepares these embeddings and additional time IDs for conditioning within the StableDiffusionXLPipeline. We then return a dictionary containing the pooled embeddings and generated time IDs, as well as the concatenated text embeddings.\n",
        "\n",
        "This choice has several advantages:\n",
        "- Richness of Representation\n",
        "- Combination of Different Models Strength\n",
        "- Reducing Overfitting \n",
        "- Enhancing Conditioning\n",
        "- Improving Performance\n",
        "\n",
        "In particular, the function receives the following **parameters**:\n",
        "- ***model***: An instance of StableDiffusionXLPipeline.\n",
        "- ***prompt***: A string containing the text prompt to be tokenized and embedded.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Get the Device**: Retrieve the device (e.g., CPU or GPU) on which the model is being executed. \n",
        "- **2) Generate Text Embeddings**: Generate text embeddings using two different pairs of tokenizers and text encoders.\n",
        "- **3) Concatenate Prompt Embeddings**: Concatenate the embeddings from the two sets of encoders along the last dimension.\n",
        "- **4) Get Text Encoder Projection Dimension**: Retrieve the projection dimension from the configuration of the second text encoder.\n",
        "- **5) Generate Additional Time IDs**: Generate additional time IDs required for conditioning.\n",
        "- **6) Prepare Additional Condition Keyword Arguments**: Checks if the input prompt is empty and handles this case.\n",
        "- **7) Return the Additional Condition Keyword Arguments and Concatenated Embeddings**: Return the prepared additional condition keyword arguments and concatenated prompt embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e2b8d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _encode_text_sdxl(model: StableDiffusionXLPipeline, prompt: str) -> tuple[dict[str, T], T]:\n",
        "    \n",
        "    # 1) Get the Device: Get the device (e.g., CPU or GPU) on which the model is being executed.\n",
        "    device = model._execution_device\n",
        "    \n",
        "    # 2) Generate Text Embeddings:\n",
        "    # Generate text embeddings using the first set of tokenizer and text encoder.\n",
        "    prompt_embeds, pooled_prompt_embeds = _get_text_embeddings(prompt, model.tokenizer, model.text_encoder, device)\n",
        "    \n",
        "    # Generate text embeddings using the second set of tokenizer and text encoder.\n",
        "    prompt_embeds_2, pooled_prompt_embeds2 = _get_text_embeddings(prompt, model.tokenizer_2, model.text_encoder_2, device)\n",
        "    \n",
        "    # 3) Concatenate Prompt Embeddings: Concatenate the embeddings from both sets of encoders along the last dimension.\n",
        "    prompt_embeds = torch.cat((prompt_embeds, prompt_embeds_2), dim=-1)\n",
        "    \n",
        "    # 4) Get Text Encoder Projection Dimension: Retrieve the projection dimension from the configuration of the second text encoder\n",
        "    text_encoder_projection_dim = model.text_encoder_2.config.projection_dim\n",
        "    \n",
        "    # 5) Generate Additional Time IDs: Generate additional time IDs required for conditioning.\n",
        "    add_time_ids = model._get_add_time_ids((1024, 1024), (0, 0), (1024, 1024), torch.float16, text_encoder_projection_dim).to(device)\n",
        "    \n",
        "    # 6) Prepare Additional Condition Keyword Arguments: Prepare additional condition keyword arguments required for the model.\n",
        "    added_cond_kwargs = {\"text_embeds\": pooled_prompt_embeds2, \"time_ids\": add_time_ids}\n",
        "    \n",
        "    # 7) Return the Additional Condition Keyword Arguments and Concatenated Embeddings:Return the prepared additional condition keyword arguments and concatenated prompt embeddings\n",
        "    return added_cond_kwargs, prompt_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e26237",
      "metadata": {},
      "source": [
        "#### 6.5: TEXT EMBEDDING: NEGATIVE CONDITIONING TECHNIQUE\n",
        "\n",
        "We then proceed to **generate Text Embeddings** and **Conditioning Keywords** for both the **Given Prompt** and an **Empty Prompt** (used for **Negative Conditioning**). We concatenate these embeddings and keywords to create a combined representation that includes both positive and negative contexts. \n",
        "\n",
        "These input prompts are used to guide the generation process in diffusion models (through **Guidance**) and improving the quality of generated outputs by considering **what the model should not generate (negative conditioning)**.\n",
        "\n",
        "The **benefits** we get from this technique are:\n",
        "- **Balanced Representation**\n",
        "- **Improved Semantic Understanding**\n",
        "- **Reduction of Noise**\n",
        "\n",
        "In particular, the function receives the following **parameters**:\n",
        "- ***model***: An instance of StableDiffusionXLPipeline.\n",
        "- ***prompt***: A string containing the text prompt to be tokenized and embedded.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Encode Text with Given Prompt using Text Embedding Ensemble**: Generate text embeddings and conditioning keywords for the given text prompt.\n",
        "- **2) Encode Text with Empty Prompt**: Generate text embeddings and conditioning keywords for an empty prompt (negative conditioning).\n",
        "- **3) Concatenate Positive and Negative Embeddings**: Concatenate the embeddings from the negative and positive prompts.\n",
        "- **4) Concatenate Positive and Negative Conditioning Keyword**: Concatenate the conditioning keywords from the negative and positive prompts.\n",
        "- **5) Return Combined Conditioning Keywords and Embeddings**: Return the combined conditioning keywords and concatenated embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6b778d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _encode_text_sdxl_with_negative(model: StableDiffusionXLPipeline, prompt: str) -> tuple[dict[str, T], T]:\n",
        "    # 1) Encode Text with Given Prompt using Text Embedding EnsembleEncode Text with Given Prompt: Generate text embeddings and conditioning keywords for the given prompt.\n",
        "    added_cond_kwargs, prompt_embeds = _encode_text_sdxl(model, prompt)\n",
        "    \n",
        "    # 2) Encode Text with Empty Prompt: Generate text embeddings and conditioning keywords for an empty prompt (negative conditioning).\n",
        "    added_cond_kwargs_uncond, prompt_embeds_uncond = _encode_text_sdxl(model, \"\")\n",
        "    \n",
        "    # 3) Concatenate Positive and Negative Embeddings: Concatenate the embeddings from the negative and positive prompts.\n",
        "    prompt_embeds = torch.cat((prompt_embeds_uncond, prompt_embeds))\n",
        "    \n",
        "    # 4) Concatenate Positive and Negative Conditioning Keywords: Concatenate the conditioning keywords from the negative and positive prompts.\n",
        "    added_cond_kwargs = {\n",
        "        \"text_embeds\": torch.cat((added_cond_kwargs_uncond[\"text_embeds\"], added_cond_kwargs[\"text_embeds\"])),\n",
        "        \"time_ids\": torch.cat((added_cond_kwargs_uncond[\"time_ids\"], added_cond_kwargs[\"time_ids\"]))\n",
        "    }\n",
        "    \n",
        "    # 5) Return Combined Conditioning Keywords and Embeddings: Return the combined conditioning keywords and concatenated embeddings.\n",
        "    return added_cond_kwargs, prompt_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e4ec2c",
      "metadata": {},
      "source": [
        "#### 6.6: ENCODE THE REFERENCE IMAGE\n",
        "\n",
        "We now switch our attention to the **Reference Image**. This is now processed and **encoded** using the **Variational Autoencoder (VAE)** of the **StableDiffusionXLPipeline**, and returns the resulting **latent representation**. The function handles the conversion of the image from a numpy array to a PyTorch tensor, normalizes it, encodes it using the VAE, and finally scales the latent representation.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Set VAE to Float32**: Ensure the VAE operates in float32 precision for encoding.\n",
        "- **2) Convert Image to PyTorch Tensor**: Convert the input image from a numpy array to a PyTorch tensor and normalize pixel values to [0, 1].\n",
        "- **3) Normalize and Prepare Image**: Scale pixel values to the range [-1, 1], rearrange dimensions, and add a batch dimension.\n",
        "- **4) Encode Image Using VAE**: Use the VAE to encode the image into the latent space.\n",
        "- **5) Reset VAE to Float16**: Optionally reset the VAE to float16 precision.\n",
        "- **6) Return Latent Representation** : Return the encoded latent representation of the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c0c336",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _encode_image(model: StableDiffusionXLPipeline, image: np.ndarray) -> T:\n",
        "    # 1) Set VAE to Float32: Ensure the VAE operates in float32 precision for encoding.\n",
        "    model.vae.to(dtype=torch.float32)\n",
        "    \n",
        "    # 2) Convert Image to PyTorch Tensor: Convert the input image from a numpy array to a PyTorch tensor and normalize pixel values to [0, 1].\n",
        "    image = torch.from_numpy(image).float() / 255.\n",
        "    \n",
        "    # 3) Normalize and Prepare Image: Scale pixel values to the range [-1, 1], rearrange dimensions, and add batch dimension.\n",
        "    image = (image * 2 - 1).permute(2, 0, 1).unsqueeze(0)\n",
        "    \n",
        "    # 4) Encode Image Using VAE: Use the VAE to encode the image into the latent space.\n",
        "    latent = model.vae.encode(image.to(model.vae.device))['latent_dist'].mean * model.vae.config.scaling_factor\n",
        "    \n",
        "    # 5) Reset VAE to Float16: Optionally reset the VAE to float16 precision.\n",
        "    model.vae.to(dtype=torch.float16)\n",
        "    \n",
        "    # 6) Return Latent Representation: Return the encoded latent representation of the image.\n",
        "    return latent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "164b2f77",
      "metadata": {},
      "source": [
        "#### 6.7: SINGLE DENOISING STEP in DIFFUSION PROCESS\n",
        "\n",
        "We now focus on performing a **Single Denoising Step** in the **Diffusion Process**. It calculates the next sample based on the current sample, the model output, and the current timestep. This function helps in progressing the denoising process step by step, gradually transforming the noisy sample back into the original image.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Calculate Current and Next Timesteps**: Compute the current and next timesteps for the denoising process.\n",
        "- **2) Calculate Alpha Products**: Retrieve the alpha cumulative product for the current and next timesteps.\n",
        "- **3) Calculate Beta Product**: Compute the beta cumulative product for the current timestep.\n",
        "- **4) Compute Next Original Sample**: Calculate the next original sample using the current sample and model output.\n",
        "- **5) Compute Next Sample Direction**: Determine the direction for the next sample.\n",
        "- **6) Compute Next Sample**: Combine the next original sample and next sample direction to get the next sample.\n",
        "- **7) Return Next Sample**: Return the computed next sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4a9a94",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _next_step(model: StableDiffusionXLPipeline, model_output: T, timestep: int, sample: T) -> T:\n",
        "    \n",
        "    # 1) Calculate Current and Next Timesteps: Compute the current and next timesteps for the denoising process.\n",
        "    timestep, next_timestep = min(timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps, 999), timestep\n",
        "    \n",
        "    # 2) Calculate Alpha Products: Retrieve the alpha cumulative product for the current and next timesteps.\n",
        "    alpha_prod_t = model.scheduler.alphas_cumprod[int(timestep)] if timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
        "    alpha_prod_t_next = model.scheduler.alphas_cumprod[int(next_timestep)]\n",
        "    \n",
        "    # 3) Calculate Beta Product: Compute the beta cumulative product for the current timestep.\n",
        "    beta_prod_t = 1 - alpha_prod_t\n",
        "    \n",
        "    # 4) Compute Next Original Sample: Calculate the next original sample using the current sample and model output.\n",
        "    next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
        "    \n",
        "    # 5) Compute Next Sample Direction: Determine the direction for the next sample.\n",
        "    next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
        "    \n",
        "    # 6) Compute Next Sample: Combine the next original sample and next sample direction to get the next sample.\n",
        "    next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
        "    \n",
        "    # 7) Return Next Sample: Return the computed next sample.\n",
        "    return next_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ae2f83",
      "metadata": {},
      "source": [
        "#### 6.8: GENERATE NOISE PREDICTIONS\n",
        "\n",
        "We now focus on generating **Noise Predictions** for the **given Latent Representation** at a specific timestep. We use a **Guidance Scale** to **combine Unconditional and Conditional Noise Predictions**, which helps guide the model in generating outputs that are closer to the desired result.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Duplicate Latent Input**: Create a batch of two identical latent representations.\n",
        "- **2) Generate Noise Predictions**: Use the model’s UNet to generate noise predictions for the duplicated latents.\n",
        "- **3) Split Noise Predictions**: Split the noise predictions into unconditional and conditional components.\n",
        "- **4) Apply Guidance**: Combine the unconditional and conditional noise predictions using the guidance scale.\n",
        "- **5) Return Noise Prediction**: Return the combined noise prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4654dac",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_noise_pred(model: StableDiffusionXLPipeline, latent: T, t: T, context: T, guidance_scale: float, added_cond_kwargs: dict[str, T]):\n",
        "    # 1) Duplicate Latent Input: Create a batch of two identical latent representations.\n",
        "    latents_input = torch.cat([latent] * 2)\n",
        "    \n",
        "    # 2) Generate Noise Predictions: Use the model's UNet to generate noise predictions for the duplicated latents.\n",
        "    noise_pred = model.unet(latents_input, t, encoder_hidden_states=context, added_cond_kwargs=added_cond_kwargs)[\"sample\"]\n",
        "    \n",
        "    # 3) Split Noise Predictions: Split the noise predictions into unconditional and conditional components.\n",
        "    noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
        "    \n",
        "    # 4) Apply Guidance: Combine the unconditional and conditional noise predictions using the guidance scale.\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
        "    \n",
        "    # 5) Return Noise Prediction: Return the combined noise prediction.\n",
        "    return noise_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce4abd4",
      "metadata": {},
      "source": [
        "#### 6.9: DDIM (Denoising Diffusion Implicit Models) Denoising Process\n",
        "\n",
        "We can now focus on the **DDIM (Denoising Diffusion Implicit Models) Denoising Process**. It iteratively refines the latent representation of an image starting from an initial noisy latent (z0), using noise predictions and timesteps to generate a sequence of progressively denoised latents. The function returns the sequence of latents from the denoising process.\n",
        "\n",
        "The steps involved are:\n",
        "- **1) Initialize Latent List**: Start with the initial latent representation (z0).\n",
        "- **2) Encode Text with Negative Conditioning**: Generate text embeddings and conditioning keywords for the prompt, including also negative conditioning.\n",
        "- **3) Prepare Latent for Inference**: Clone and detach the initial latent, and convert it to half precision.\n",
        "- **4) Denoising Loop**: Perform the denoising process over the specified number of inference steps.\n",
        "\t- **4.1) Get Current Timestep**: Retrieve the current timestep.\n",
        "\t- **4.2) Generate Noise Prediction**: Use the model to predict noise for the current latent and timestep.\n",
        "\t- **4.3) Compute Next Latent**: Compute the next latent representation using the noise prediction.\n",
        "\t- **4.4) Append Latent to List**: Append the new latent to the list of all latents.\n",
        "- **5) Return Sequence of Latents**: Concatenate all latents and reverse their order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3aeb12",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _ddim_loop(model: StableDiffusionXLPipeline, z0, prompt, guidance_scale) -> T:\n",
        "    # 1) Initialize Latent List: Start with the initial latent representation.\n",
        "    all_latent = [z0]\n",
        "    \n",
        "    # 2) Encode Text with Negative Conditioning: Generate text embeddings and conditioning keywords for the prompt, including also negative conditioning.\n",
        "    added_cond_kwargs, text_embedding = _encode_text_sdxl_with_negative(model, prompt)\n",
        "    \n",
        "    # 3) Prepare Latent for Inference: Clone and detach the initial latent, and convert it to half precision.\n",
        "    latent = z0.clone().detach().half()\n",
        "    \n",
        "    # 4) Denoising Loop: Perform the denoising process over the specified number of inference steps.\n",
        "    for i in tqdm(range(model.scheduler.num_inference_steps)):\n",
        "        # 4.1) Get Current Timestep: Retrieve the current timestep.\n",
        "        t = model.scheduler.timesteps[len(model.scheduler.timesteps) - i - 1]\n",
        "        \n",
        "        # 4.2) Generate Noise Prediction: Use the model to predict noise for the current latent and timestep.\n",
        "        noise_pred = _get_noise_pred(model, latent, t, text_embedding, guidance_scale, added_cond_kwargs)\n",
        "        \n",
        "        # 4.3) Compute Next Latent: Compute the next latent representation using the noise prediction.\n",
        "        latent = _next_step(model, noise_pred, t, latent)\n",
        "        \n",
        "        # 4.4) Append Latent to List: Append the new latent to the list of all latents.\n",
        "        all_latent.append(latent)\n",
        "    \n",
        "    # 5) Return Sequence of Latents: Concatenate all latents and reverse their order.\n",
        "    return torch.cat(all_latent).flip(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3221446d",
      "metadata": {},
      "source": [
        "#### 6.10:  UPDATE LATENT TENSORS CALLBACK\n",
        "\n",
        "We focus now on creating a **Callback Function** that **updates the Latent Tensors** during the **Diffusion Process**. This callback is used to replace the current latent with a precomputed latent at each step, ensuring that the diffusion process follows a predefined path.\n",
        "\n",
        "The steps involved are:\n",
        "- **1) Define Callback Function**: Define a callback function to be called at the end of each diffusion step.\n",
        "\t- **1.1) Retrieve Current Latents**: Extract the current latents from the callback arguments.\n",
        "\t- **1.2) Update Latent**: Replace the first latent with a precomputed latent from the zts list.\n",
        "\t- **1.3) Return Updated Latents**: Return the updated latents.\n",
        "- **2) Return Initial Latent and Callback**: Return the initial latent and the callback function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a16164",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_inversion_callback(zts, offset: int = 0) -> [T, InversionCallback]:\n",
        "    # 1) Define Callback Function: Define a callback function to be called at the end of each diffusion step.\n",
        "    def callback_on_step_end(pipeline: StableDiffusionXLPipeline, i: int, t: T, callback_kwargs: dict[str, T]) -> dict[str, T]:\n",
        "        # 1.1) Retrieve Current Latents: Extract the current latents from the callback arguments.\n",
        "        latents = callback_kwargs['latents']\n",
        "        \n",
        "        # 1.2) Update Latent: Replace the first latent with a precomputed latent from the zts list.\n",
        "        latents[0] = zts[max(offset + 1, i + 1)].to(latents.device, latents.dtype)\n",
        "        \n",
        "        # 1.3) Return Updated Latents: Return the updated latents.\n",
        "        return {'latents': latents}\n",
        "    \n",
        "    # 2) Return Initial Latent and Callback: Return the initial latent and the callback function.\n",
        "    return zts[offset], callback_on_step_end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc89d45",
      "metadata": {},
      "source": [
        "#### 6.11: DDIM Inversion Process\n",
        "\n",
        "We can now put all together and perform the the **DDIM (Denoising Diffusion Implicit Models) Inversion Process**. It starts by encoding the input image into a latent representation, sets the timesteps for the diffusion process, and then iteratively refines the latent representation using the DDIM loop. The final output is a sequence of latent representations.\n",
        "\n",
        "The steps involved are: \n",
        "- **1) Encode Image**: Encode the input image into a latent representation using the model’s VAE.\n",
        "- **2) Set Timesteps**: Set the timesteps for the diffusion process.\n",
        "- **3) Perform DDIM Loop**: Perform the DDIM denoising loop to generate a sequence of latent representations.\n",
        "- **4) Return Sequence of Latents**: Return the sequence of latent representations generated by the DDIM loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b8cae1",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ddim_inversion(model: StableDiffusionXLPipeline, x0: np.ndarray, prompt: str, num_inference_steps: int, guidance_scale: float) -> T:\n",
        "    # 1) Encode Image: Encode the input image into a latent representation using the model's VAE.\n",
        "    z0 = _encode_image(model, x0)\n",
        "    \n",
        "    # 2) Set Timesteps: Set the timesteps for the diffusion process.\n",
        "    model.scheduler.set_timesteps(num_inference_steps, device=z0.device)\n",
        "    \n",
        "    # 3) Perform DDIM Loop: Perform the DDIM denoising loop to generate a sequence of latent representations.\n",
        "    zs = _ddim_loop(model, z0, prompt, guidance_scale)\n",
        "    \n",
        "    # 4) Return Sequence of Latents: Return the sequence of latent representations generated by the DDIM loop.\n",
        "    return zs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72621c0e",
      "metadata": {
        "id": "72621c0e"
      },
      "source": [
        "#### 7: DIFFUSION INVERSION PROCESS MAIN\n",
        "\n",
        "\n",
        "In this section, we will undergo the **Diffusion Inversion Process** in order to **align the style of generated images with a reference style**. This process involves several key steps:\n",
        "\n",
        "\n",
        "1. **Set Prompts**:\n",
        "   - We define a set of prompts for generating images. The first prompt refers to the reference image, while the subsequent prompts are used to generate new images.\n",
        "   - The reference style is appended to each of the subsequent prompts to ensure the generated images adhere to the desired style.\n",
        "\n",
        "2. **Configure Style Alignment Handler**:\n",
        "   - We initialize a handler for the Style Aligned (SA) model and configure it using the `StyleAlignedArgs` parameters. These parameters control various aspects of the style alignment process, such as normalization and attention mechanisms.\n",
        "\n",
        "3. **Run Diffusion Inversion**:\n",
        "   - We execute the DDIM inversion process to map the reference image to its latent representation. This inversion allows us to extract latent features that can be used to guide the generation of new images in the desired style.\n",
        "\n",
        "4. **Generate Images**:\n",
        "   - Using the latent representation obtained from the inversion process, we generate new images based on the defined prompts. We will generate random latent vectors of shape (number_of_prompts, 4, 128, 128) from a normal distribution. We will make use of generator to ensure that the random values are reproducible. In this step, I will have also to ensure that the latent vectors have the same data type as required by the model’s UNet.\n",
        "   After this, we will set the first latent vector to the one extracted from the reference image, ensuring that the first generated image closely adheres to the reference style.\n",
        "   The latent features of the reference image are combined with the prompts to produce images that are stylistically aligned with the reference image.\n",
        "\n",
        "5. **Display Results**:\n",
        "   - Finally, we display the generated images to visualize the effect of the style alignment.\n",
        "\n",
        "This process demonstrates how to leverage the power of diffusion models and inversion techniques to generate images with consistent and coherent styles, guided by a reference image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b030817",
      "metadata": {
        "id": "6b030817"
      },
      "outputs": [],
      "source": [
        "# Set of prompts to generate images for. The first refers to the Reference Image. The other to generate images.\n",
        "prompts = [\n",
        "    reference_prompt,\n",
        "    \"A man working on a laptop\",\n",
        "    \"A man eats pizza\",\n",
        "    \"A woman playig on saxophone\",\n",
        "]\n",
        "\n",
        "# Append the reference style to each of subsequent prompts for generating images with the same Style.\n",
        "for i in range(1, len(prompts)):\n",
        "    prompts[i] = f'{prompts[i]}, {reference_style}.'\n",
        "\n",
        "# Configure the StyleAligned Handler using the StyleAlignedArgs.\n",
        "handler = sa_handler.Handler(SDXL_Pipeline)\n",
        "sa_args = sa_handler.StyleAlignedArgs(\n",
        "    share_group_norm=True,\n",
        "    share_layer_norm=True,\n",
        "    share_attention=True,\n",
        "    adain_queries=True,\n",
        "    adain_keys=True,\n",
        "    adain_values=False,\n",
        "    style_alignment_score_shift=np.log(style_alignment_score_shift),\n",
        "    style_alignment_score_scale=style_alignment_score_scale)\n",
        "handler.register(sa_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecb36ed",
      "metadata": {
        "id": "5ecb36ed"
      },
      "outputs": [],
      "source": [
        "# Execute the Diffusion Inversion Process to map the reference image to its latent representation.\n",
        "DDIM_inv_result = inversion.ddim_inversion(SDXL_Pipeline, ref_image, reference_prompt, num_inference_steps, 2)\n",
        "\n",
        "# Extract the latent representation from the Diffusion Inversion Result that can be used to guide the generation of new images in the desired style.\n",
        "latent_vector_ref_img, inversion_callback = inversion.make_inversion_callback(DDIM_inv_result, offset=5)\n",
        "\n",
        "# Create a Random Number Generator on the CPU.\n",
        "rand_gen = torch.Generator(device='cpu').manual_seed(31)\n",
        "\n",
        "# Generate the images using the latent representation of the reference image as guidance.\n",
        "latents = torch.randn(len(prompts), 4, 128, 128,            # Random Latent Vectors shape\n",
        "                      device='cpu',                         # Latent Vectors on CPU.\n",
        "                      generator=rand_gen,                   # Random Number Generator.\n",
        "                      dtype=SDXL_Pipeline.unet.dtype,).to('cuda:0') # Data Type of the Latent Vectors (same as required by the model's UNet).\n",
        "\n",
        "# Set the first latent vector to the latent representation of the reference image extracted before.\n",
        "latents[0] = latent_vector_ref_img\n",
        "\n",
        "# Generate the images using the provided prompts and the latent vectors.\n",
        "images_a = SDXL_Pipeline(\n",
        "    prompts,                                 # Prompts to generate images for.\n",
        "    latents=latents,                         # Latent Vectors to guide the generation of images.\n",
        "    callback_on_step_end=inversion_callback, # Callback to update the latent vectors during the generation process.\n",
        "    num_inference_steps=num_inference_steps, # Number of Inference Steps to generate the images.\n",
        "    guidance_scale=guidance_scale).images              # Guidance Scale to control the influence of the latent vectors on the generated images.\n",
        "\n",
        "# Display the generated images.\n",
        "handler.remove()\n",
        "mediapy.show_images(images_a, titles=[p[:-(len(reference_style) + 3)] for p in prompts])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0372abb9ee8e472c97e6413bbf98861c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "06f5d7f994aa43388b7020976a0b65f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c6deca986341cebc4cd8954081600b",
            "placeholder": "​",
            "style": "IPY_MODEL_659ff92781d94d2796896cb50aead054",
            "value": ""
          }
        },
        "1250b69ca7854136a792a2fc6280197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "169c007fe65c4963bc27aab517b24cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ddcfcc473fe434c96afec08ab6d4c91",
            "placeholder": "​",
            "style": "IPY_MODEL_354313f61a1c4b6dae6c04379863f839",
            "value": ""
          }
        },
        "170a2fd37187478ab8efbe8d7a23cc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ddcfcc473fe434c96afec08ab6d4c91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "354313f61a1c4b6dae6c04379863f839": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca2f1f6040d4d20937812ed2dee8dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "559605b074094805b43e8eac048cf910": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06f5d7f994aa43388b7020976a0b65f1",
              "IPY_MODEL_7a12797fa2f94e9692168907384544eb",
              "IPY_MODEL_62de0ccd03344706ab8221f5bd77277c"
            ],
            "layout": "IPY_MODEL_981afaa972a34bb9ad01028a75c80d36"
          }
        },
        "5f7997cd2b0b4136a86e3be938138ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_169c007fe65c4963bc27aab517b24cb8",
              "IPY_MODEL_995f9f48a02f498f90306eebcb2cda8e",
              "IPY_MODEL_f5eaa36e07c148c3a670a983fa34d2ea"
            ],
            "layout": "IPY_MODEL_fc1629ba1f0442da9187f7c431317091"
          }
        },
        "62de0ccd03344706ab8221f5bd77277c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db7772e6feb4574be618cadfbfbbb70",
            "placeholder": "​",
            "style": "IPY_MODEL_f89d8573f0fa4982bf41dd5dcaa33079",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "659ff92781d94d2796896cb50aead054": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7426a510d3a64b31b4172ed67de9ca01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a12797fa2f94e9692168907384544eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0372abb9ee8e472c97e6413bbf98861c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_170a2fd37187478ab8efbe8d7a23cc1e",
            "value": 0
          }
        },
        "8db7772e6feb4574be618cadfbfbbb70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981afaa972a34bb9ad01028a75c80d36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "995f9f48a02f498f90306eebcb2cda8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1250b69ca7854136a792a2fc6280197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7426a510d3a64b31b4172ed67de9ca01",
            "value": 0
          }
        },
        "a1c6deca986341cebc4cd8954081600b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f351d052c36848c3b15a50c0e13c9e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5eaa36e07c148c3a670a983fa34d2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca2f1f6040d4d20937812ed2dee8dd5",
            "placeholder": "​",
            "style": "IPY_MODEL_f351d052c36848c3b15a50c0e13c9e68",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "f89d8573f0fa4982bf41dd5dcaa33079": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc1629ba1f0442da9187f7c431317091": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
