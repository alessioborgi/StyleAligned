{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540d8642-c203-471c-a66d-0d43aabb0706",
   "metadata": {},
   "source": [
    "# StyleAligned: Zero-Shot Style Alignment among a Series of Generated Images via Attention Sharing\n",
    "\n",
    "### **Authors**: ***Borgi Alessio***, ***Danese Francesco***\n",
    "\n",
    "### **Abstract**\n",
    "In this notebook we aim to reproduce and enhance **[StyleAligned](https://arxiv.org/abs/2312.02133)**, a novel technique introduced by **Google Research**, for achieving **Style Consistency** in large-scale Text-to-Image (T2I) generative models. While current T2I models excel in creating visually compelling images from textual descriptions, they often struggle to maintain a consistent style across multiple images. Traditional methods to address this require extensive fine-tuning and manual intervention. \n",
    "\n",
    "**StyleAligned** addresses this challenge by introducing minimal **Attention Sharing** during the **Diffusion Process**, ensuring **Style Alignment among generated images** without the need for optimization or fine-tuning (**Zero-Shoot Inference**). The method operates by leveraging a straightforward inversion operation to apply a reference style across various generated images, maintaining high-quality synthesis and fidelity to the provided text prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82b07b",
   "metadata": {},
   "source": [
    "### 0: SETTINGS & IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07f09c",
   "metadata": {},
   "source": [
    "#### 0.1: CLONE REPOSITORY AND GIT SETUP\n",
    "\n",
    "In the following cell, we setup the code, by cloning the repository, setting up the Git configurations, and providing some other useful commands useful for git.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd StyleAlignedDiffModels\n",
    "%ls\n",
    "\n",
    "# Set up Git configuration\n",
    "!git config --global user.name \"Alessio Borgi\"\n",
    "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
    "\n",
    "# Stage the changes\n",
    "#!git add .\n",
    "\n",
    "# Commit the changes\n",
    "#!git commit -m \"Added some content to your-file.txt\"\n",
    "\n",
    "# Push the changes (replace 'your-token' with your actual personal access token)\n",
    "#!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f941e8d",
   "metadata": {},
   "source": [
    "#### 0.2: INSTALL AND IMPORT REQUIRED LIBRARIES\n",
    "\n",
    "We proceed then by installing and importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94180e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d54ea7-f7ab-4548-9b10-ece87216dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, DDIMScheduler\n",
    "import torch\n",
    "import mediapy\n",
    "import sa_handler\n",
    "from diffusers.utils import load_image\n",
    "import inversion\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734c603",
   "metadata": {},
   "source": [
    "### 4: DDIM SCHEDULER\n",
    "\n",
    "We then proceed to load the **SDXL (Stable Diffusion XL)** Model and configure the **DDIM (Denoising Diffusion Implicit Models) Scheduler**.\n",
    "\n",
    "The **DDIM Scheduler** is the component used in diffusion models for generating high-quality samples from noise. It controls the denoising process by defining a schedule for adding and removing noise to and from the data. The scheduler is essential in determining how the model transitions from pure noise to a final, coherent image or other data form.\n",
    "\n",
    "In particular, its parameters are:\n",
    "- **beta_start (float)**: Starting value of beta, the variance of the noise schedule. \n",
    "- **beta_end (float)**: Ending value of beta, the variance of the noise schedule. \n",
    "- **beta_schedule (str)**: The type of schedule for beta. (Possible values: \"linear\", \"scaled_linear\", \"squaredcos_cap_v2\", \"sigmoid\"). \n",
    "- **clip_sample (bool)**: If True, the samples are clipped to [-1, 1]. \n",
    "- **set_alpha_to_one (bool)**: If True, alpha will be set to 1 at the end of the sampling process.\n",
    "- **num_train_timesteps (int)**: The number of diffusion steps used during training. \n",
    "- **timestep_spacing (str)**: The method to space out timesteps.(Possible values: \"linspace\", \"leading\"). \n",
    "\t•\tprediction_type (str): The type of prediction model used in the scheduler. Possible values: \"epsilon\", \"sample\", \"v-prediction\". Default: \"epsilon\"\n",
    "\t•\ttrained_betas (torch.Tensor or None): Optional tensor of pre-trained betas to use in the scheduler. Default: None\n",
    "\n",
    "\n",
    "## DDIM Scheduler\n",
    "\n",
    "The **DDIMScheduler** (Denoising Diffusion Implicit Models Scheduler) is used in diffusion models for generating high-quality samples from noise. It controls the denoising process by defining a schedule for adding and removing noise to and from the data.\n",
    "\n",
    "### Key Parameters and their Descriptions\n",
    "\n",
    "- **beta_start**: Starting value of beta, the variance of the noise schedule.\n",
    "- **beta_end**: Ending value of beta, the variance of the noise schedule.\n",
    "- **beta_schedule**: The type of schedule for beta values.\n",
    "- **clip_sample**: If `True`, the samples are clipped to [-1, 1].\n",
    "- **set_alpha_to_one**: If `True`, alpha will be set to 1 at the end of the sampling process.\n",
    "- **num_train_timesteps**: The number of diffusion steps used during training.\n",
    "- **timestep_spacing**: The method to space out timesteps.\n",
    "- **prediction_type**: The type of prediction model used in the scheduler.\n",
    "- **trained_betas**: Optional tensor of pre-trained betas to use in the scheduler.\n",
    "\n",
    "### Diffusion Process\n",
    "\n",
    "The diffusion process involves adding noise to the data over a series of timesteps, which is described by the forward process:\n",
    "\n",
    "\\[ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\]\n",
    "\n",
    "where:\n",
    "- \\( \\alpha_t \\) and \\( \\beta_t \\) are the scaling and noise variance terms, respectively.\n",
    "\n",
    "### Reverse Process\n",
    "\n",
    "The reverse process aims to recover the data by denoising it, and is given by:\n",
    "\n",
    "\\[ p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I}) \\]\n",
    "\n",
    "where:\n",
    "- \\( \\mu_{\\theta}(\\mathbf{x}_t, t) \\) is the predicted mean.\n",
    "- \\( \\sigma_t \\) is the standard deviation of the noise at timestep \\( t \\).\n",
    "\n",
    "### Beta Schedule\n",
    "\n",
    "The beta values are scheduled over timesteps from `beta_start` to `beta_end`. The schedule can be:\n",
    "- **Linear**: \n",
    "\n",
    "\\[ \\beta_t = \\beta_{\\text{start}} + t \\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T} \\]\n",
    "\n",
    "- **Scaled Linear**:\n",
    "\n",
    "\\[ \\beta_t = \\beta_{\\text{start}} + t \\left(\\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T}\\right)^2 \\]\n",
    "\n",
    "- **Sigmoid**:\n",
    "\n",
    "\\[ \\beta_t = \\beta_{\\text{start}} + (\\beta_{\\text{end}} - \\beta_{\\text{start}}) \\cdot \\text{sigmoid}(t) \\]\n",
    "\n",
    "### Inference with DDIM\n",
    "\n",
    "During inference, the denoising process can be described as:\n",
    "\n",
    "\\[ \\mathbf{x}_{t-1} = \\sqrt{\\alpha_{t-1}} \\left( \\frac{\\mathbf{x}_t - \\sqrt{1 - \\alpha_t} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)}{\\sqrt{\\alpha_t}} \\right) + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) \\]\n",
    "\n",
    "where:\n",
    "- \\( \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) \\) is the noise predicted by the model.\n",
    "- \\( \\sigma_t \\) is the standard deviation for the timestep \\( t \\).\n",
    "\n",
    "### Example Initialization\n",
    "\n",
    "Here is an example initialization of the DDIMScheduler with all available parameters and their default values:\n",
    "\n",
    "```python\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,                  # Starting value of beta\n",
    "    beta_end=0.012,                     # Ending value of beta\n",
    "    beta_schedule=\"scaled_linear\",      # Type of schedule for beta values\n",
    "    clip_sample=False,                  # Whether to clip samples to a specified range\n",
    "    set_alpha_to_one=False,             # Whether to set alpha to one at the end of the process\n",
    "    num_train_timesteps=1000,           # Number of diffusion steps used during training\n",
    "    timestep_spacing=\"linspace\",        # Method to space out timesteps\n",
    "    prediction_type=\"epsilon\",          # Type of prediction model used in the scheduler\n",
    "    trained_betas=None                  # Optional pre-trained beta values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6f1e6-445f-47bc-b9db-0301caeb7490",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# init models\n",
    "\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085, \n",
    "    beta_end=0.012, \n",
    "    beta_schedule=\"scaled_linear\", \n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False)\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,                 # Starting value of beta\n",
    "    beta_end=0.012,                     # Ending value of beta\n",
    "    beta_schedule=\"scaled_linear\",      # Type of schedule for beta values\n",
    "    clip_sample=False,                  # Whether to clip samples to a specified range\n",
    "    set_alpha_to_one=False,             # Whether to set alpha to one at the end of the process\n",
    "    \n",
    "    num_train_timesteps=1000,           # Number of diffusion steps used during training\n",
    "    timestep_spacing=\"linspace\",        # Method to space out timesteps\n",
    "    prediction_type=\"epsilon\",          # Type of prediction model used in the scheduler\n",
    "    trained_betas=None                  # Optional pre-trained beta values\n",
    ")\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\", \n",
    "    use_safetensors=True,\n",
    "    scheduler=scheduler\n",
    ").to(\"cuda\")\n",
    "\n",
    "handler = sa_handler.Handler(pipeline)\n",
    "sa_args = sa_handler.StyleAlignedArgs(share_group_norm=False,\n",
    "                                      share_layer_norm=False,\n",
    "                                      share_attention=True,\n",
    "                                      adain_queries=True,\n",
    "                                      adain_keys=True,\n",
    "                                      adain_values=False\n",
    "                                     )\n",
    "\n",
    "handler.register(sa_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd6159",
   "metadata": {},
   "source": [
    "### 5: RUNNING STYLE-ALIGNED with A SET OF PROMPTS WITHOUT REFERENCE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca9256-0ce0-45c3-9cba-68c7eff1452f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run StyleAligned\n",
    "\n",
    "sets_of_prompts = [\n",
    "  \"a toy train. macro photo. 3d game asset\",\n",
    "  \"a toy airplane. macro photo. 3d game asset\",\n",
    "  \"a toy bicycle. macro photo. 3d game asset\",\n",
    "  \"a toy car. macro photo. 3d game asset\",\n",
    "  \"a toy boat. macro photo. 3d game asset\",\n",
    "]\n",
    "images = pipeline(sets_of_prompts,).images\n",
    "mediapy.show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819ad6d-0c19-411f-ba97-199909f64805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run StyleAligned\n",
    "sets_of_prompts = [\n",
    "  \"a toy train. macro photo. 3d game asset\",\n",
    "  \"a toy airplane. macro photo. 3d game asset\",\n",
    "  \"a toy bicycle. macro photo. 3d game asset\",\n",
    "  \"a toy car. macro photo. 3d game asset\",\n",
    "  \"a toy boat. macro photo. 3d game asset\",\n",
    "]\n",
    "# sets_of_prompts = [\n",
    "#   \"a hot hair balloon, simple wooden statue\",\n",
    "#   \"a friendly robot, simple wooden statue\",\n",
    "#   \"a bull, simple wooden statue\",\n",
    "# ]\n",
    "images = []\n",
    "for prompt in sets_of_prompts:\n",
    "    # Generate image for each prompt individually\n",
    "    image = pipeline([prompt]).images[0]\n",
    "    images.append(image)\n",
    "    # Clear CUDA cache to free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print Memory summary\n",
    "    # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    \n",
    "mediapy.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1bd22",
   "metadata": {},
   "source": [
    "### 6: STYLE-ALIGNED WITH REFERENCE IMAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e473def",
   "metadata": {},
   "source": [
    "#### 6.1: LOADING AND INVERTING REFERENCE IMAGE  \n",
    "Load a reference image and perform the inversion process to extract latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d034bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_style = \"medieval painting\"\n",
    "src_prompt = f'Man laying in a bed, {src_style}.'\n",
    "image_path = './imgs/medieval-bed.jpeg'\n",
    "\n",
    "num_inference_steps = 50\n",
    "x0 = np.array(load_image(image_path).resize((1024, 1024)))\n",
    "zts = inversion.ddim_inversion(pipeline, x0, src_prompt, num_inference_steps, 2)\n",
    "mediapy.show_image(x0, title=\"innput reference image\", height=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
