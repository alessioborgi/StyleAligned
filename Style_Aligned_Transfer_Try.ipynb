{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd StyleAlignedDiffModels\n",
    "%ls\n",
    "\n",
    "# Set up Git configuration\n",
    "!git config --global user.name \"Alessio Borgi\"\n",
    "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
    "\n",
    "# Stage the changes\n",
    "#!git add .\n",
    "\n",
    "# Commit the changes\n",
    "#!git commit -m \"Added some content to your-file.txt\"\n",
    "\n",
    "# Push the changes (replace 'your-token' with your actual personal access token)\n",
    "#!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import einops\n",
    "import mediapy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "from typing import Callable\n",
    "from dataclasses import dataclass\n",
    "from __future__ import annotations\n",
    "from diffusers.utils import load_image\n",
    "from torch.nn import functional as nnf\n",
    "from diffusers.models import attention_processor\n",
    "from diffusers.image_processor import PipelineImageInput\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from diffusers.utils.torch_utils import is_compiled_module, is_torch_version\n",
    "from diffusers import StableDiffusionXLPipeline, DDIMScheduler, ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "\n",
    "import sa_handler_try as sa_handler\n",
    "T = torch.tensor # Create Alias for torch.tensor to increase readability.\n",
    "TN = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False)\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    scheduler=scheduler\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Medieval Painting\n",
    "# Set the source style, prompt and path.\n",
    "reference_style = \"medieval painting\"\n",
    "reference_prompt = f'Man laying in a bed, {reference_style}.'\n",
    "reference_image_path = './imgs/medieval-bed.jpeg'\n",
    "\n",
    "# 2) Cubism Painting\n",
    "# reference_style = \"cubism painting\"\n",
    "# reference_prompt = f'Two men smoking water pipe, {reference_style}.'\n",
    "# reference_image_path = './imgs/Picasso_Smoking_Water_Pipe.jpeg'\n",
    "\n",
    "\n",
    "# Setting the number of inference steps in the Diffusion Inversion Process.\n",
    "num_inference_steps = 50\n",
    "\n",
    "# Setting the Guidance Scale for the Diffusion Inversion Process.\n",
    "guidance_scale = 10.0\n",
    "\n",
    "# 1) Normal Painting\n",
    "# These are some parameters you can Adjust to Control StyleAlignment to Reference Image.\n",
    "style_alignment_score_shift = 2  # higher value induces higher fidelity, set 0 for no shift\n",
    "style_alignment_score_scale = 1.0  # higher value induces higher, set 1 for no rescale\n",
    "\n",
    "# 2) Very Famous Paintings\n",
    "# style_alignment_score_shift = 1\n",
    "# style_alignment_score_scale = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference image and resize it to 1024x1024 pixels.\n",
    "ref_image = np.array(load_image(reference_image_path).resize((1024, 1024)))\n",
    "\n",
    "# Display the output image.\n",
    "mediapy.show_image(ref_image, title=\"Reference Image for Style Alignment\", height=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a type alias for the Diffusion Inversion Process type of callable.\n",
    "Diff_Inversion_Process_Callback = Callable[[StableDiffusionXLPipeline, int, T, dict[str, T]], dict[str, T]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_tokenizazion_and_embedding(prompt: str, tokenizer, text_encoder, device):\n",
    "\n",
    "    # 1) Tokenize the Input Prompt: Tokenize the input prompt using the provided tokenizer, with padding and truncation.\n",
    "    prompt_tokenized = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\n",
    "    # 2) Extract Token IDs: Extract the input IDs (token indices) from the tokenized inputs.\n",
    "    prompt_tokenized_ids = prompt_tokenized.input_ids\n",
    "\n",
    "    # 3) Generate Embeddings: Use torch.no_grad() to disable gradient computation for the following operations.\n",
    "    with torch.no_grad():\n",
    "        # Generate embeddings for the tokenized input IDs using the text encoder.\n",
    "        # The embeddings include output hidden states.\n",
    "        prompt_embeddings = text_encoder(\n",
    "            prompt_tokenized_ids.to(device),  # Move input IDs to the specified device (e.g., GPU).\n",
    "            output_hidden_states=True,  # Request hidden states from the encoder.\n",
    "        )\n",
    "\n",
    "    # 4) Extract Pooled Output Embeddings: Extract the pooled output embeddings (first element of the tuple returned by the encoder).\n",
    "    pooled_prompt_embeddings = prompt_embeddings[0]\n",
    "    # 5) Extract Hidden State Embeddings: Extract the hidden state embeddings from the second last layer of the encoder.\n",
    "    prompt_embeddings = prompt_embeddings.hidden_states[-2]\n",
    "\n",
    "    # 6) Handle Empty Prompt Case: If the prompt is empty, return zero tensors as Negative Embeddings.\n",
    "    if prompt == '':\n",
    "        # Create a zero tensor with the same shape as the hidden state embeddings.\n",
    "        negative_prompt_embeddings = torch.zeros_like(prompt_embeddings)\n",
    "        # Create a zero tensor with the same shape as the pooled output embeddings.\n",
    "        negative_pooled_prompt_embeddings = torch.zeros_like(pooled_prompt_embeddings)\n",
    "        # Return the zero tensors for both negative embeddings and pooled negative embeddings.\n",
    "        return negative_prompt_embeddings, negative_pooled_prompt_embeddings\n",
    "\n",
    "    # 7) Returns the generated embeddings: Return the hidden state embeddings and the pooled output embeddings.\n",
    "    return prompt_embeddings, pooled_prompt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_ensemble(model: StableDiffusionXLPipeline, prompt: str) -> tuple[dict[str, T], T]:\n",
    "\n",
    "    # 1) Get the Device: Get the device (e.g., CPU or GPU) on which the model is being executed.\n",
    "    device = model._execution_device\n",
    "\n",
    "    # 2) Generate Text Embeddings:\n",
    "    # Generate text embeddings using the first set of tokenizer and text encoder.\n",
    "    prompt_embeddings_1, pooled_prompt_embeddings_1 = prompt_tokenizazion_and_embedding(prompt, model.tokenizer, model.text_encoder, device)\n",
    "\n",
    "    # Generate text embeddings using the second set of tokenizer and text encoder.\n",
    "    prompt_embeddings_2, pooled_prompt_embeddings_2 = prompt_tokenizazion_and_embedding(prompt, model.tokenizer_2, model.text_encoder_2, device)\n",
    "\n",
    "    # 3) Concatenate Prompt Embeddings: Concatenate the embeddings from both sets of encoders along the last dimension.\n",
    "    prompt_embeddings_concat = torch.cat((prompt_embeddings_1, prompt_embeddings_2), dim=-1)\n",
    "\n",
    "    # 4) Get Text Encoder Projection Dimension: Retrieve the projection dimension from the configuration of the second text encoder\n",
    "    prompt_encoder_projection_dim = model.text_encoder_2.config.projection_dim\n",
    "\n",
    "    # 5) Generate Additional Time IDs: Generate additional time IDs required for conditioning.\n",
    "    conditioning_time_ids = model._get_add_time_ids((1024, 1024), (0, 0), (1024, 1024), torch.float16, prompt_encoder_projection_dim).to(device)\n",
    "\n",
    "    # 6) Prepare Additional Condition Keyword Arguments: Prepare additional condition keyword arguments required for the model.\n",
    "    conditioning_kwargs = {\"text_embeds\": pooled_prompt_embeddings_2, \"time_ids\": conditioning_time_ids}\n",
    "\n",
    "    # 7) Return the Additional Condition Keyword Arguments and Concatenated Embeddings:Return the prepared additional condition keyword arguments and concatenated prompt embeddings\n",
    "    return conditioning_kwargs, prompt_embeddings_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_ensemble_with_neg_conditioning(model: StableDiffusionXLPipeline, prompt: str) -> tuple[dict[str, T], T]:\n",
    "    # 1) Encode Text with Given Prompt using Text Embedding Ensemble Encode Text with Given Prompt: Generate text embeddings and conditioning keywords for the given prompt.\n",
    "    conditioning_kwargs, prompt_embeddings_concat = embeddings_ensemble(model, prompt)\n",
    "\n",
    "    # 2) Encode Text with Empty Prompt: Generate text embeddings and conditioning keywords for an empty prompt (negative conditioning).\n",
    "    unconditioning_kwargs, prompt_embeddings_uncond = embeddings_ensemble(model, \"\")\n",
    "\n",
    "    # 3) Concatenate Positive and Negative Embeddings: Concatenate the embeddings from the negative and positive prompts.\n",
    "    prompt_embeddings_concat = torch.cat((prompt_embeddings_uncond, prompt_embeddings_concat))\n",
    "\n",
    "    # 4) Concatenate Positive and Negative Conditioning Keywords: Concatenate the conditioning keywords from the negative and positive prompts.\n",
    "    conditioning_unconditioning_kwargs = {\n",
    "        \"text_embeds\": torch.cat((unconditioning_kwargs[\"text_embeds\"], conditioning_kwargs[\"text_embeds\"])),\n",
    "        \"time_ids\": torch.cat((unconditioning_kwargs[\"time_ids\"], conditioning_kwargs[\"time_ids\"]))\n",
    "    }\n",
    "\n",
    "    # 5) Return Combined Conditioning Keywords and Embeddings: Return the combined conditioning keywords and concatenated embeddings.\n",
    "    return conditioning_unconditioning_kwargs, prompt_embeddings_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encoding(model: StableDiffusionXLPipeline, image: np.ndarray) -> T:\n",
    "\n",
    "    # 1) Set VAE to Float32: Ensure the VAE operates in float32 precision for encoding.\n",
    "    model.vae.to(dtype=torch.float32)\n",
    "\n",
    "    # 2) Convert Image to PyTorch Tensor: Convert the input image from a numpy array to a PyTorch tensor and normalize pixel values to [0, 1].\n",
    "    scaled_img = torch.from_numpy(image).float() / 255.\n",
    "\n",
    "    # 3) Normalize and Prepare Image: Scale pixel values to the range [-1, 1], rearrange dimensions, and add batch dimension.\n",
    "    permuted_img = (scaled_img * 2 - 1).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    # 4) Encode Image Using VAE: Use the VAE to encode the image into the latent space.\n",
    "    latent_img = model.vae.encode(permuted_img.to(model.vae.device))['latent_dist'].mean * model.vae.config.scaling_factor\n",
    "\n",
    "    # 5) Reset VAE to Float16: Optionally reset the VAE to float16 precision.\n",
    "    model.vae.to(dtype=torch.float16)\n",
    "\n",
    "    # 6) Return Latent Representation: Return the encoded latent representation of the image.\n",
    "    return latent_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Denoising_next_step(model: StableDiffusionXLPipeline, model_output: T, timestep: int, sample: T) -> T:\n",
    "\n",
    "    # 1) Calculate Current and Next Timesteps: Compute the current and next timesteps for the denoising process.\n",
    "    current_timestep, next_timestep = min(timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps, 999), timestep\n",
    "\n",
    "    # 2) Calculate Alpha Products: Retrieve the alpha cumulative product for the current and next timesteps.\n",
    "    alpha_prod_t = model.scheduler.alphas_cumprod[int(current_timestep)] if current_timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
    "    alpha_prod_t_next = model.scheduler.alphas_cumprod[int(next_timestep)]\n",
    "\n",
    "    # 3) Calculate Beta Product: Compute the beta cumulative product for the current timestep.\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "    # 4) Compute Next Original Sample: Calculate the next original sample using the current sample and model output.\n",
    "    next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "\n",
    "    # 5) Compute Next Sample Direction: Determine the direction for the next sample.\n",
    "    next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "\n",
    "    # 6) Compute Next Sample: Combine the next original sample and next sample direction to get the next sample.\n",
    "    next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "\n",
    "    # 7) Return Next Sample: Return the computed next sample.\n",
    "    return next_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Noise_Prediction(model: StableDiffusionXLPipeline, latent: T, t: T, context: T, guidance_scale: float, added_cond_kwargs: dict[str, T]):\n",
    "    # 1) Duplicate Latent Input: Create a batch of two identical latent representations.\n",
    "    double_input_latents = torch.cat([latent] * 2)\n",
    "\n",
    "    # 2) Generate Noise Predictions: Use the model's UNet to generate noise predictions for the duplicated latents.\n",
    "    noise_prediction = model.unet(double_input_latents, t, encoder_hidden_states=context, added_cond_kwargs=added_cond_kwargs)[\"sample\"]\n",
    "\n",
    "    # 3) Split Noise Predictions: Split the noise predictions into unconditional and conditional components.\n",
    "    noise_prediction_unconditioned, noise_prediction_text = noise_prediction.chunk(2)\n",
    "\n",
    "    # 4) Apply Guidance: Combine the unconditional and conditional noise predictions using the guidance scale.\n",
    "    noise_prediction = noise_prediction_unconditioned + guidance_scale * (noise_prediction_text - noise_prediction_unconditioned)\n",
    "\n",
    "    # 5) Return Noise Prediction: Return the combined noise prediction.\n",
    "    return noise_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Denoising_Process(model: StableDiffusionXLPipeline, z0, prompt, guidance_scale) -> T:\n",
    "    # 1) Initialize Latent List: Start with the initial latent representation.\n",
    "    latent_list = [z0]\n",
    "\n",
    "    # 2) Encode Text with Negative Conditioning: Generate text embeddings and conditioning keywords for the prompt, including also negative conditioning.\n",
    "    conditioning_unconditioning_kwargs, prompt_embedding = embeddings_ensemble_with_neg_conditioning(model, prompt)\n",
    "\n",
    "    # 3) Prepare Latent for Inference: Clone and detach the initial latent, and convert it to half precision.\n",
    "    latent = z0.clone().detach().half()\n",
    "\n",
    "    # 4) Denoising Loop: Perform the denoising process over the specified number of inference steps.\n",
    "    for i in tqdm(range(model.scheduler.num_inference_steps)):\n",
    "        # 4.1) Get Current Timestep: Retrieve the current timestep.\n",
    "        current_timestep = model.scheduler.timesteps[len(model.scheduler.timesteps) - i - 1]\n",
    "\n",
    "        # 4.2) Generate Noise Prediction: Use the model to predict noise for the current latent and timestep.\n",
    "        noise_prediction = Generate_Noise_Prediction(model, latent, current_timestep, prompt_embedding, guidance_scale, conditioning_unconditioning_kwargs)\n",
    "\n",
    "        # 4.3) Compute Next Latent: Compute the next latent representation using the noise prediction.\n",
    "        next_latent = Denoising_next_step(model, noise_prediction, current_timestep, latent)\n",
    "\n",
    "        # 4.4) Append Latent to List: Append the new latent to the list of all latents.\n",
    "        latent_list.append(next_latent)\n",
    "\n",
    "    # 5) Return Sequence of Latents: Concatenate all latents and reverse their order.\n",
    "    final_latent = torch.cat(latent_list).flip(0)\n",
    "\n",
    "    return final_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_and_inversion(ddim_result, offset: int = 0) -> [T, Diff_Inversion_Process_Callback]:\n",
    "\n",
    "    def callback_on_step_end(pipeline: StableDiffusionXLPipeline, i: int, t: T, callback_kwargs: dict[str, T]) -> dict[str, T]:\n",
    "\n",
    "        latents = callback_kwargs['latents']\n",
    "        # Update the first latent tensor with the corresponding tensor from ddim_result.\n",
    "        latents[0] = ddim_result[max(offset + 1, i + 1)].to(latents.device, latents.dtype)\n",
    "        return {'latents': latents}\n",
    "\n",
    "    # Return the initial latent tensor and the callback function.\n",
    "    return  ddim_result[offset], callback_on_step_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def DDIM_Inversion_Process(model: StableDiffusionXLPipeline, x0: np.ndarray, prompt: str, num_inference_steps: int, guidance_scale: float) -> T:\n",
    "\n",
    "    # 1) Encode Image: Encode the input image into a latent representation using the model's VAE.\n",
    "    encoded_img = image_encoding(model, x0)\n",
    "\n",
    "    # 2) Set Timesteps: Set the timesteps for the diffusion process.\n",
    "    model.scheduler.set_timesteps(num_inference_steps, device=encoded_img.device)\n",
    "\n",
    "    # 3) Perform DDIM Loop: Perform the DDIM denoising loop to generate a sequence of latent representations.\n",
    "    latent_repr_sequence = Denoising_Process(model, encoded_img, prompt, guidance_scale)\n",
    "\n",
    "    # 4) Return Sequence of Latents: Return the sequence of latent representations generated by the DDIM loop.\n",
    "    return latent_repr_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of prompts to generate images for. The first refers to the Reference Image. The other to generate images.\n",
    "prompts = [\n",
    "    reference_prompt,\n",
    "    \"A man working on a laptop\",\n",
    "    \"A man eats pizza\",\n",
    "    \"A woman playig on saxophone\",\n",
    "]\n",
    "\n",
    "# Append the reference style to each of subsequent prompts for generating images with the same Style.\n",
    "for i in range(1, len(prompts)):\n",
    "    prompts[i] = f'{prompts[i]}, {reference_style}.'\n",
    "\n",
    "# Configure the StyleAligned Handler using the StyleAlignedArgs.\n",
    "handler = sa_handler.Handler(pipeline)\n",
    "sa_args = sa_handler.StyleAlignedArgs(\n",
    "    share_group_norm=True,\n",
    "    share_layer_norm=True,\n",
    "    share_attention=True,\n",
    "    adain_queries=True,\n",
    "    adain_keys=True,\n",
    "    adain_values=False,\n",
    "    style_alignment_score_shift=np.log(style_alignment_score_shift),\n",
    "    style_alignment_score_scale=style_alignment_score_scale)\n",
    "handler.register(sa_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Diffusion Inversion Process to map the reference image to its latent representation.\n",
    "DDIM_inv_result = DDIM_Inversion_Process(pipeline, ref_image, reference_prompt, num_inference_steps, 2)\n",
    "\n",
    "# Extract the latent representation from the Diffusion Inversion Result that can be used to guide the generation of new images in the desired style.\n",
    "latent_vector_ref_img, inversion_callback = extract_latent_and_inversion(DDIM_inv_result, offset=5)\n",
    "\n",
    "# Create a Random Number Generator on the CPU.\n",
    "rand_gen = torch.Generator(device='cpu').manual_seed(31)\n",
    "\n",
    "# Generate the images using the latent representation of the reference image as guidance.\n",
    "latents = torch.randn(len(prompts), 4, 128, 128,            # Random Latent Vectors shape\n",
    "                      device='cpu',                         # Latent Vectors on CPU.\n",
    "                      generator=rand_gen,                   # Random Number Generator.\n",
    "                      dtype=pipeline.unet.dtype,).to('cuda:0') # Data Type of the Latent Vectors (same as required by the model's UNet).\n",
    "\n",
    "# Set the first latent vector to the latent representation of the reference image extracted before.\n",
    "latents[0] = latent_vector_ref_img\n",
    "\n",
    "# Generate the images using the provided prompts and the latent vectors.\n",
    "images_a = pipeline(\n",
    "    prompts,                                 # Prompts to generate images for.\n",
    "    latents=latents,                         # Latent Vectors to guide the generation of images.\n",
    "    callback_on_step_end=inversion_callback, # Callback to update the latent vectors during the generation process.\n",
    "    num_inference_steps=num_inference_steps, # Number of Inference Steps to generate the images.\n",
    "    guidance_scale=guidance_scale).images              # Guidance Scale to control the influence of the latent vectors on the generated images.\n",
    "\n",
    "# Display the generated images.\n",
    "handler.remove()\n",
    "mediapy.show_images(images_a, titles=[p[:-(len(reference_style) + 3)] for p in prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
